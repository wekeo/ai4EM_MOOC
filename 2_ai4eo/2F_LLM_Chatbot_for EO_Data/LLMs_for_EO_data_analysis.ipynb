{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9dc39e3d",
      "metadata": {},
      "source": [
        "<center><img src='img/ai4eo_logos.jpg' alt='Logos AI4EO MOOC' width='80%'></img></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd8f0b1",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ed0961",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19dc7f91",
      "metadata": {},
      "source": [
        "<a href='https://www.futurelearn.com/courses/artificial-intelligence-for-earth-monitoring/1/steps/1291928' target='_blank'><< Back to FutureLearn</a><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcfad93a",
      "metadata": {},
      "source": [
        "# A Chatbot for Questions about the Earth "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "742ae7be",
      "metadata": {},
      "source": [
        "<i>by Anna-Lena Erdmann, EUMETSAT, Darmstadt, Germany</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a58262b",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81166cfe",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d1d26f",
      "metadata": {},
      "source": [
        "*Imagine you can ask any question about the Earth - and get a data-driven answer*\n",
        "\n",
        "By the end of this notebook, you will learn how WEkEO tools can be used to create a chatbot that can answer user questions about sea surface temperatures by analyzing the underlying Earth Observation (EO) data: "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e707c8fd",
      "metadata": {},
      "source": [
        "<left><img src='img/knoweo_webinterface.png' alt='knoweo Web Interface' width='50%'></img></left>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a5ff052",
      "metadata": {},
      "source": [
        "This is possible by **combining EO data analysis tools of WEkEO with Large Language Models**. \n",
        "\n",
        "\n",
        "**Large Language Models** and applications built on LLMs have experienced an immense boost of popularity since the launch of Open AI's <a href=\"https://chatgpt.com/\">ChatGPT</a> in 2023.\n",
        "LLMs are artificial intelligence foundation models, which have been trained on massive amounts of data with billions of parameters. Being trained on large amounts of text data, LLMs are capable of processing and generating text and generalize to multiple tasks [[Naveed et al. 2023]](#Naveedetal2023). \n",
        "\n",
        " When it comes to **specialized** tasks, which require any type of information they have not been trained on, the usage of LLM soon reaches its limit. Examples where LLMs are reaching its limit are: \n",
        " - accessing recent information from after they have been trained\n",
        " - making complex mathematical computations\n",
        " - writing code for a not well-documented programming language \n",
        "\n",
        "Accessing and analyzing EO data is a really specialized task, which requires expert knowledge of EO data formats, access APIs, processing tools, and visualization tools. For non-experts, it is challenging to analyze Earth Observation data to get insights about the Earth. \n",
        "\n",
        "<a href=\"https://wekeo.eu/\">WEkEO</a> provides a **harmonized data access** to all data from the Copernicus programm. Data Access is simplified through a number of tools, such as the HDA API and climetlab plugins. Being able to access and process many different earth observation datasets through one platform reduces the complexity of EO data access and analysis. \n",
        "\n",
        "This notebook explores the **capability of LLMs to access and analyze EO data** to answer user questions about the Earth. Equipped with the right tools coming from WEkEO, an LLM generates data-driven answers about the Earth on natural Language questions from anyone. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4246f6c0",
      "metadata": {},
      "source": [
        "## What this Notebook will cover\n",
        "\n",
        "* [1 - Introduction to LLMs and LangChain](#intro)\n",
        "* [2 - Creating a Sea Surface Temperature Chatbot](#sstchatbot)\n",
        "     * [2.1 - Defining the LLM](#sstagent)\n",
        "     * [2.2 - Creating the EO toolbox](#ssttoolbox)\n",
        "     * [2.3 - Initializing the Agent](#iniagent)\n",
        "     * [2.4 - Asking questions and receiving answers](#sstqa)\n",
        "* [3 - Conclusions](#conclusions)\n",
        "* [4 - Additional Resources](#resources)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04e80ff2",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd0b44f",
      "metadata": {},
      "source": [
        "## <a id='intro'></a> 1 - Introduction to LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c25ad255",
      "metadata": {},
      "source": [
        "### LLMs: \n",
        "\n",
        "Large Language Models are aritificial intelligence foundation models, that can process and generate text. They have recently enjoyed an increase in capability, and following this popularity, mainly because of advances in transformer networks, increased computational capabilities, and the availability of large-scale training data [[Naveed et al. 2023]](#Naveedetal2023). \n",
        "\n",
        "LLMs are transformer neural networks, first described in the paper \"Attention Is All You Need\" [[Vaswani et al 2017]](#Vasvanietal2017). Transformers are sequence-to-sequence networks designed to predict an output sequence based on an input sequence. Applications of transformers are in translation task, or text completion tasks. In the use case of text completion task, based on an input sequence the LLM predicts the probability distribution of the next word (or token).  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7838a41d",
      "metadata": {},
      "source": [
        "Giving the LLM a prompt, e.g. a task description or a user question as input sequence, the LLM will predict the next word. This word is added to the input sequence and acts as a new input sequence, where the subsequent word is predicted, added to the input sequence, and so on. This is repeated until a full paragraph is finished and the user will finally receive a chatGPT-like answer from the LLM. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed2c944",
      "metadata": {},
      "source": [
        "<left><img src='img/token_prediction.png' alt='token prediction' width='50%'></img></left>\n",
        "\n",
        "Image source: [Mishra 2024][]\n",
        "\n",
        "[Mishra 2024]:https://www.linkedin.com/pulse/how-do-language-modelsllm-work-we-call-chatgpt-mishra-fdqsc/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cb89940",
      "metadata": {},
      "source": [
        "\n",
        "### Augmented LLMs: \n",
        "\n",
        "While pre-trained LLMs are masters of generalization and practically all-round talents, they soon reach their limits for specialized tasks. There are different ways of specializing pre-trained LLMs [[Naveed et al. 2023]](#Naveedetal2023): \n",
        "\n",
        "- *fine-tuning*: re-open the \"box\" of weights of the LLM and **retrain it on specialized training data** the LLM needs to serve the specialized task. Fine-tuning usually has a high demand for computational resources. \n",
        "\n",
        "- *LLM augmentation*: augmentation means to equip the LLM with **external resources** that help the LLM to solve the required task. Only new information is added without altering the original LLM. This method is quite efficient for specializing LLMs without getting too deep into the process of training and validation of large neural network architectures. \n",
        "\n",
        "Ways of augmenting LLMs: \n",
        "\n",
        "*Retrieval-augmented generation (RAG) technique* This technique adds a so-called retriever next to the LLM that retrieves information and provides this information to the LLM. The LLM has more context specifically related to the user question and can therefore rely on external sources of information in addition to the information the LLM was trained on. This external information can be e.g. a database with specific documentation or a web search. \n",
        "\n",
        "\n",
        "*Tool augmented LLMs* While the RAG technique relies on the retriever to provide information to the LLM, tool augmentation relies on the LLM-inherent reasoning capabilities. The LLMs can divide tasks into sub-tasks and address the right tools to solve the sub-tasks. One example is the RestGPT, which provides an LLM with prompted API documentation so that the LLM can create queries for the REST APIs based on their documentation [[Song et al. 2023]](#Songetal2023). \n",
        "\n",
        "<left><img src='img/rag.png' alt='RAG' width='35%'></img></left>\n",
        "<left><img src='img/tool.png' alt='Tool' width='32%'></img></left>\n",
        "\n",
        "Image source: [[Naveed et al. 2023]](#Naveedetal2023)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24cdd098",
      "metadata": {},
      "source": [
        "\n",
        "### AI Agents \n",
        "\n",
        "[Xi et al. 2023](#Xietal2023) describe AI agents as \"**artificial entities that sense their environment, make\n",
        "decisions, and take actions**.\" While earlier AI agents were rule-based, the capabilities of LLM make them predestined as flexible and adaptable **\"brain\" of agents**. \n",
        "As the brain of the agent, the LLM steers the workflow from the user question to the solution by dividing the task, selecting which tools to choose from, and reacts properly if something goes wrong. An agent can also consist of many LLMs, from which one is the brain, abd the other are other \"personas\" focusing on one specific task, such as code generation. \n",
        "\n",
        "<left><img src='img/agent_framework.png' alt='Agent' width='45%'></img></left>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e41c15f7",
      "metadata": {},
      "source": [
        "Using frameworks such as the ReAct [[Yao et al. 2023]](#Yaoetal2023), the LLM undergoes a chain of thought, action, and evaluation. This image shows an example of the ReAct framework and how it triggers the LLM to act as the brain of the agent.\n",
        "\n",
        "This notebook makes use of the ReAct framework to create an AI agent with OpenAIs gpt-4 as LLM \"brain\". Using WEkEO tools, together with additional tools, such as python executors or code templates, a toolbox is created which enables the LLM to solve geospatial analysis tasks on earth observation data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae2b34ad",
      "metadata": {},
      "source": [
        "### Langchain 🦜️🔗 \n",
        "\n",
        "To build the AI Agent and create our LLM-powered application this notebook uses LangChain. <a href=\"https://github.com/langchain-ai/langchain\">LangChain</a> is a powerful open-source framework that simplifies the modular build-up of LLM applications. Through the LAngChain expression language and abstraction modules between the LLM and the developer, LangChain enables a simple and fast augmentation of LLMs with additional data sources and even the creation of LLM-powered agents. The full documentation of LangChain is available <a href=\"https://python.langchain.com/v0.2/docs/introduction/\">here</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8861d6ed",
      "metadata": {},
      "source": [
        "## <a id='sstchatbot'></a> 2 - Creating a Sea Surface Temperature Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6209739",
      "metadata": {},
      "source": [
        "The goal of this notebook is to provide an example of how data access tools of WEkEO can be used to create LLM applications for data analysis tasks. While acting as an experiment and example, the scope of the AI agent is limited to the following capabilities: \n",
        "\n",
        "- this notebook only develops the LLM agent - **not the web interface**\n",
        "- it is only possible to analyze **one dataset** on the topic of sea surface temperature\n",
        "- the data for analysis is downloaded in the background: the execution will fail for user questions where **massive amounts of data** are used and should be limited on questions for specific days (max. one week)\n",
        "-  EO workflow consists of **recurring modules**: \n",
        "   - identification which data is needed to answer the question\n",
        "   - download of the data\n",
        "   - understanding the data and data variables\n",
        "   - analyzing the data\n",
        "- create a **specialized chatbot**, that knows this procedure and has information on the questions that can be answered with the dataset. For remaining questions, the chatbot can use internal knowledge. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72ed418",
      "metadata": {},
      "source": [
        "### <a id='sstsetup'></a> 2.0 - Setting up the Python Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8493e1f6",
      "metadata": {},
      "source": [
        "Install the packages which are required for execitung this notebook. \n",
        "\n",
        "If you have already set up the environment using the **environment.yaml** file provided with the notebook, or have launched this notebook inside the **WEkEO JupyterHub**, you can skip this step. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7c4669da",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-text-splitters 0.0.1 requires langchain-core<0.2.0,>=0.1.28, but you have langchain-core 0.0.13 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip -q install langchain\n",
        "%pip -q install climetlab-wekeo-source\n",
        "%pip -q install climetlab-wekeo-datasets\n",
        "%pip -q install climetlab\n",
        "%pip install -q zarr\n",
        "%pip install -q xarray[complete]\n",
        "%pip install -q langchain-experimental\n",
        "%pip install -q openai==0.27.4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef07794e",
      "metadata": {},
      "source": [
        "Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "921abc84",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# LLM + Langchain packages\n",
        "from langchain import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.agents import Tool\n",
        "from langchain.tools import BaseTool\n",
        "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
        "\n",
        "# data analysis packages\n",
        "import climetlab as cml\n",
        "from climetlab_wekeo_datasets import hda2cml\n",
        "import xarray as xr\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b6d89fd",
      "metadata": {},
      "source": [
        "We are using OpenAI's GPT 4 for this notebook. You need an API key for this. Set your OpenAI key as enviroment variable. If you don't have an OpenAI API key, you can create a user account [here](https://openai.com/). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8d72ab4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02b1c1a",
      "metadata": {},
      "source": [
        "### <a id='sstagent'></a> 2.1 - Defining the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72b1d5aa",
      "metadata": {},
      "source": [
        "We are using the OpenAI's GPT-4 model for this example. You can also substitute this model by other LLMs if prefered. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8dce262e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the LLM - OpenAIs GPT4\n",
        "turbo_llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-4'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5167b702",
      "metadata": {},
      "source": [
        "### <a id='ssttoolbox'></a> 2.2 - Creating the EO toolbox"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d666ca8",
      "metadata": {},
      "source": [
        "As described in Section 2, an AI Agent needs access to **specific tools** for problem solving. In this section we define the necessary tools for the data access and analysis. The tools are built of 2 parts: \n",
        "\n",
        "1. The **Description**: The description is exposed to the LLM. Based on the description of the tool, the LLM will determine wheter the tool is suitable for the specific sub-task. It is important that the description described which input is necessary to the tool and which output can be expected after the tool is executed. \n",
        "\n",
        "2. The **Function Definition**: The function defined the actual processes which are triggered when the tool is executed by the LLM. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6626e1c8",
      "metadata": {},
      "source": [
        "#### Tool 1: API Request Creator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "15dbcb8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class APICreator(BaseTool):\n",
        "    name = \"Create API Request\"\n",
        "    description = \"Generates an API request for satellite data given a dictionary of 'start_time' in the format \\\"YYYY-MM-DDT00:00:00.000Z\\\", 'end_time' in the format \\\"YYYY-MM-DDT00:00:00.000Z\\\" and 'dataset_id'. Please wrap the json in a string\"\n",
        "    def _run(self, input: str):\n",
        "        input_dict = json.loads(input)\n",
        "\n",
        "        api_template = '''{\n",
        "                      \"dataset_id\": \"EO:EUM:DAT:METOP:GLB-SST-NC\",\n",
        "                      \"dtstart\": {start_time},\n",
        "                      \"dtend\": {end_time}\n",
        "                    }'''\n",
        "        api_request =  api_template.replace(\"{start_time}\", '\"'+input_dict['start_time']+'\"').replace(\"{end_time}\", '\"'+input_dict['end_time']+'\"')\n",
        "        return api_request.replace('{', '{{').replace('}', '}}')\n",
        "\n",
        "\n",
        "    def _arun(self, input: list):\n",
        "        raise NotImplementedError(\"This tool does not support async\")\n",
        "\n",
        "api_creator = APICreator()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae419fd",
      "metadata": {},
      "source": [
        "#### Tool 2: Data Downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0e12d4f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataDownloader(BaseTool):\n",
        "    name = \"Download Data and get Shape\"\n",
        "    description = \"The input to the tool is an API request created by the APICreator tool! Given this API request as a json wrapped in a string, this tool downloads the data, converts it to an xarray and gives back the shape of the data. This tool is useful if you want to find out which python code is needed next to analyze the data.\"\n",
        "    def _run(self, input: str):\n",
        "        global api_request_glob\n",
        "        api_request_glob = input\n",
        "        input_dict = json.loads(input)\n",
        "        dsid, args = hda2cml(input_dict)\n",
        "        cml_ds = cml.load_dataset(dsid, **args)\n",
        "        ds_local = cml_ds.to_xarray()\n",
        "        output = \"Coordinates of xarray Dataset: \", ds_local.coords, \" Variables: \", list(ds_local.keys())\n",
        "        return output\n",
        "\n",
        "\n",
        "    def _arun(self, input: list):\n",
        "        raise NotImplementedError(\"This tool does not support async\")\n",
        "\n",
        "downloader = DataDownloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6700b2b3",
      "metadata": {},
      "source": [
        "#### Tool 3: Python Executor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "271c1878",
      "metadata": {},
      "outputs": [],
      "source": [
        "python_exe = PythonAstREPLTool()\n",
        "\n",
        "python_tool = Tool(\n",
        "        name = \"python executer\",\n",
        "        func=python_exe,\n",
        "        description=\"takes python code as input and executes it. all packages have to be imported inside the python code. make sure that relevant findings are printed to be further processed.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed514c29",
      "metadata": {},
      "source": [
        "#### Tool 4: Data Analyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "074d6804",
      "metadata": {},
      "outputs": [],
      "source": [
        "analysis_cmlcode = '''import climetlab as cml\n",
        "import xarray as xr\n",
        "import json\n",
        "from climetlab_wekeo_datasets import hda2cml\n",
        "dsid, args = hda2cml(api_request)\n",
        "cml_ds = cml.load_dataset(dsid, **args)\n",
        "ds_local = cml_ds.to_xarray()\n",
        "\n",
        "'''\n",
        "\n",
        "class DataAnalyzer(BaseTool):\n",
        "    name = \"Climetlab Data Analyzer\"\n",
        "    description = \"The input to the tool is valid python code to make data analysis. This tool is useful after downloading the data from an API to answer the user question. The data is a xarray dataset and is stored in the variable ds_local. It is most likely a das-sliced xarray, so always use the python package xarray to make the calculations and  the function.compute()in the end! Print the final result using the print() statement.\"\n",
        "    def _run(self, input: str):\n",
        "        code = analysis_cmlcode.replace(\"api_request\", api_request_glob)\n",
        "        print(code)\n",
        "        return  python_exe.run(code+input)\n",
        "\n",
        "\n",
        "analysis_tool = DataAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "880670d6",
      "metadata": {},
      "source": [
        "### <a id='iniagent'></a> 2.3 - Setting up the Agent "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6a33d4",
      "metadata": {},
      "source": [
        "Now, that we have defined the tools, we can initialize the agent and equit it with the tools from the EO toolbox. \n",
        "\n",
        "Memory is added to the agent, so it can have a conversational interaction with the user and can answer follow-up questions from the context of the conversation. The nomber of interactions the LLM can memorize is set by the parameter k and is set to 3. \n",
        "\n",
        "The conversational Agent is initialized through the function ``initialize_agent`` with the following parameters: \n",
        "\n",
        "``agent`` defined the agent prompt. We use the ReAct description. \n",
        "\n",
        "``tools`` equips the agent with the tools from the EO toolboy we created.\n",
        "\n",
        "`` llm`` sets the used LLM\n",
        "\n",
        "`` verbose`` decides whether we want to see the output of the LLM during the task solving\n",
        "\n",
        "``max_iterations`` lets the LLM iterate 5 times between thought and action, this prevents an infinite loop of thoughts and reactions of the LLM \n",
        "\n",
        "`` memory`` sets the conversational memory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ccb23d14",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import SystemMessagePromptTemplate\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# conversational agent memory\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=3,\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "tools = [downloader, api_creator,  analysis_tool, python_tool]\n",
        "\n",
        "conversational_agent = initialize_agent(\n",
        "    agent='chat-conversational-react-description',\n",
        "    tools=tools,\n",
        "    llm=turbo_llm,\n",
        "    verbose=True,\n",
        "    max_iterations=5,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17409fd2",
      "metadata": {},
      "source": [
        "### Prompt Engineering: Telling the LLM which tasks it has to solve\n",
        "\n",
        "Each LLM has a prompt which defines the \"persona\" of the LLM. More precisely, the LLM prompt determines the context in which the LLM acts. The prompt is part of the input secuence that determined which next work the LLM predicts. Therefore it is very important to adjust the prompt in a way that it fits to the task of the LLM. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc4b358",
      "metadata": {},
      "source": [
        "First, explore the default prompt of the LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cbe8fad6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.'))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.agent.llm_chain.prompt.messages[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b7df228",
      "metadata": {},
      "source": [
        "Next, we define out custom prompt, which fits to the task the LLM should solve and overwrite the default prompt with out custom persona prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4a8fa712",
      "metadata": {},
      "outputs": [],
      "source": [
        "fixed_prompt = '''Assistant is a large language model trained by OpenAI.\n",
        "\n",
        "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions on specific datasets of Earth Observation Data. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
        "\n",
        "Assistant doesn't know anything about API requests and how they are built up.\n",
        "\n",
        "Assistant also doesn't know information about the shape of datasets or od sea surface temperature.\n",
        "\n",
        "Assistant knows that the usual way of earth observation data analysis follows the three steps: generating an API request, download the data using the API request, analysing data using the climetlab_python tool.\n",
        "\n",
        "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "88bef0d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "conversational_agent.agent.llm_chain.prompt.messages[0].prompt.template = fixed_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e482b070",
      "metadata": {},
      "source": [
        "We also adjust the prompt describing the available tools and required outputs to make the agent less prone to formating errors when generating input to the tools. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "280a76e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "fixed_human_prompt = '''TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Download Data and get Shape: The input to the tool is an API request created by the APICreator tool! Given this API request as a json wrapped in a string, this tool downloads the data, converts it to an xarray and gives back the shape of the data. This tool is useful if you want to find out which python code is needed next to analyze the data.\\n> Create API Request: Generates an API request for satellite data given a dictionary of \\'start_time\\' in the format \"YYYY-MM-DDT00:00:00.000Z\", \\'end_time\\' in the format \"YYYY-MM-DDT00:00:00.000Z\" and \\'dataset_id\\'. Please wrap the json in a string\\n> Climetlab Data Analyzer: The input to the tool is valid python code to make data analysis. This tool is useful after downloading the data from an API to answer the user question. The data is a xarray dataset and is stored in the variable ds_local. It is most likely a das-sliced xarray, so always use the python package xarray to make the calculations and  the function.compute()in the end! Print the final result using the print() statement.\\n> python executer: takes python code as input and executes it. all packages have to be imported inside the python code. make sure that relevant findings are printed to be further processed.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of Download Data and get Shape, Create API Request, Climetlab Data Analyzer, python executer\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}}\\n``` Under no cicumstances use more than one markdown snippet in one response, under no circumstances print python code in '```' markdown snippets. \\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}'''\n",
        "\n",
        "conversational_agent.agent.llm_chain.prompt.messages[2].prompt.template = fixed_human_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5cd7c70",
      "metadata": {},
      "source": [
        "### <a id='sstqa'></a> 2.4 - Asking questions and receiving answers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19c4cc96",
      "metadata": {},
      "source": [
        "Now we are ready to ask questions on Sea Surface Temperature to the Agent. The output illustrated the chain of thought the agent undergoes while solving the Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0469e338",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer this question, we first need to generate an API request to fetch the sea surface temperature data for the Mediterranean Sea on Christmas Eve, 2023. We will use the \"Create API Request\" tool for this. The dataset_id will depend on the specific dataset you are using for sea surface temperature data. For example, if you are using the Copernicus Marine Environment Monitoring Service (CMEMS) dataset, the dataset_id might be something like \"SST_MED_SST_L4_NRT_OBSERVATIONS_010_004\". Please replace \"dataset_id\" with the correct id for your dataset.\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"action\": \"Create API Request\",\n",
            "    \"action_input\": \"{\\\"start_time\\\": \\\"2023-12-24T00:00:00.000Z\\\", \\\"end_time\\\": \\\"2023-12-24T23:59:59.000Z\\\", \\\"dataset_id\\\": \\\"dataset_id\\\"}\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m{{\n",
            "                      \"dataset_id\": \"EO:EUM:DAT:METOP:GLB-SST-NC\",\n",
            "                      \"dtstart\": \"2023-12-24T00:00:00.000Z\",\n",
            "                      \"dtend\": \"2023-12-24T23:59:59.000Z\"\n",
            "                    }}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe API request for the sea surface temperature data over the Mediterranean Sea for Christmas Eve, 2023 has been successfully created. The dataset we are using is \"EO:EUM:DAT:METOP:GLB-SST-NC\". The start time is \"2023-12-24T00:00:00.000Z\" and the end time is \"2023-12-24T23:59:59.000Z\". \n",
            "\n",
            "Now, we need to download the data using this API request and get the shape of the data. We will use the \"Download Data and get Shape\" tool for this. \n",
            "\n",
            "```json\n",
            "{\n",
            "    \"action\": \"Download Data and get Shape\",\n",
            "    \"action_input\": \"{\\\"dataset_id\\\": \\\"EO:EUM:DAT:METOP:GLB-SST-NC\\\", \\\"dtstart\\\": \\\"2023-12-24T00:00:00.000Z\\\", \\\"dtend\\\": \\\"2023-12-24T23:59:59.000Z\\\"}\"\n",
            "}\n",
            "```\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3m('Coordinates of xarray Dataset: ', Coordinates:\n",
            "  * time     (time) datetime64[ns] 24B 2023-12-24 2023-12-24T12:00:00 2023-12-25\n",
            "  * lat      (lat) float32 14kB -89.97 -89.92 -89.88 ... 89.88 89.92 89.97\n",
            "  * lon      (lon) float32 29kB -180.0 -179.9 -179.9 ... 179.9 179.9 180.0, ' Variables: ', ['sea_surface_temperature', 'sst_dtime', 'sses_bias', 'sses_standard_deviation', 'dt_analysis', 'wind_speed', 'sea_ice_fraction', 'aerosol_dynamic_indicator', 'adi_dtime_from_sst', 'sources_of_adi', 'l2p_flags', 'quality_level', 'satellite_zenith_angle', 'solar_zenith_angle'])\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe data for the sea surface temperature over the Mediterranean Sea for Christmas Eve, 2023 has been successfully downloaded. The data is in the form of an xarray Dataset with the following coordinates and variables:\n",
            "\n",
            "Coordinates:\n",
            "- time: 2023-12-24, 2023-12-24T12:00:00, 2023-12-25\n",
            "- lat: Ranges from -89.97 to 89.97\n",
            "- lon: Ranges from -180.0 to 180.0\n",
            "\n",
            "Variables include 'sea_surface_temperature', 'sst_dtime', 'sses_bias', 'sses_standard_deviation', 'dt_analysis', 'wind_speed', 'sea_ice_fraction', 'aerosol_dynamic_indicator', 'adi_dtime_from_sst', 'sources_of_adi', 'l2p_flags', 'quality_level', 'satellite_zenith_angle', 'solar_zenith_angle'.\n",
            "\n",
            "Now, we need to analyze this data to find the sea surface temperature over the Mediterranean Sea for Christmas Eve, 2023. We will use the \"Climetlab Data Analyzer\" tool for this. The exact code will depend on the specific boundaries of the Mediterranean Sea in terms of latitude and longitude. For example, the latitude might range from 30 to 45 degrees and the longitude might range from -6 to 36 degrees. Please replace \"lat_min\", \"lat_max\", \"lon_min\", and \"lon_max\" with the correct values for your dataset.\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"action\": \"Climetlab Data Analyzer\",\n",
            "    \"action_input\": \"import xarray as xr\\n\\n# Define the boundaries of the Mediterranean Sea\\nlat_min, lat_max = 30, 45\\nlon_min, lon_max = -6, 36\\n\\n# Select the data for the Mediterranean Sea and for Christmas Eve, 2023\\nmediterranean_sst = ds_local.sel(time='2023-12-24', lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))['sea_surface_temperature']\\n\\n# Calculate the mean sea surface temperature\\nmean_sst = mediterranean_sst.mean().compute()\\n\\nprint(mean_sst)\"\n",
            "}\n",
            "```\u001b[0mimport climetlab as cml\n",
            "import xarray as xr\n",
            "import json\n",
            "from climetlab_wekeo_datasets import hda2cml\n",
            "dsid, args = hda2cml({\"dataset_id\": \"EO:EUM:DAT:METOP:GLB-SST-NC\", \"dtstart\": \"2023-12-24T00:00:00.000Z\", \"dtend\": \"2023-12-24T23:59:59.000Z\"})\n",
            "cml_ds = cml.load_dataset(dsid, **args)\n",
            "ds_local = cml_ds.to_xarray()\n",
            "\n",
            "\n",
            "\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m<xarray.DataArray 'sea_surface_temperature' ()> Size: 8B\n",
            "array(290.42190034)\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe average sea surface temperature over the Mediterranean Sea for Christmas Eve, 2023, was approximately 290.42 Kelvin.\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"The average sea surface temperature over the Mediterranean Sea for Christmas Eve, 2023, was approximately 290.42 Kelvin.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The average sea surface temperature over the Mediterranean Sea for Christmas Eve, 2023, was approximately 290.42 Kelvin.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"What was the sea surface temperature over the mediterranean sea for Christmas eve last year (2023)?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b02e84",
      "metadata": {},
      "source": [
        "The LLM successfully chains together the defined tools for API request creation, data download, data exploration and, data analysis. With the code templates given to the LLM in the tools and its inherent knowledge of modules such as xarray, the LLM successfully calculates the average sea surface temperature for the given day. \n",
        "\n",
        "Our applications' response to the user: \n",
        "\n",
        "**'The average sea surface temperature over the Mediterranean Sea for Christmas Eve, 2023, was approximately 290.42 Kelvin.'**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d50d302",
      "metadata": {},
      "source": [
        "## <a id='conclusions'></a> 3. Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbfecbee",
      "metadata": {},
      "source": [
        "Using an **pre-trained LLM and Earth Observation data access tools**, this notebook creates an application capable of data-driven answering of questions about the Earth. While it shows great potential for future extensions, some **limitations** and potential **improvements**  need to be discussed:\n",
        "\n",
        "1. the scope of the application is **limited to one dataset only**. Therefore, the application does not contain a module that can select specific datasets for a user question. This can get tricky when there is more than one dataset available for the same topic, or the datasets are not ideally documented. To extend this application with a **\"data selection\"** module, the next step could be to include RAG retreivals to the agent, supplying the LLM with information on the potential datasets with the user question. \n",
        "\n",
        "2. **Usage of tokens**. The ReAct and Agent framework is really powerful for self-contained task solving of the LLM. However, one must keep in mind that it causes a high number of input+context tokens (~ 7000 for one user question) and several rounds of LLM predictions (one for each new thought). This makes the presented application **costly**. Using the OpenAI gtp-4 model API, each user question cumulates to around 0.25$. This cost can be reduced by reducing the input prompts, which can come at the cost of the abilities of the LLM. Substituting the (relatively costly) OpenAI gpt-4 API with the gpt-3.5-turbo API or APIs for open-source models could further reduce the cost. \n",
        "\n",
        "3. **Code generation**. The custom tools rely heavily on **fixed code templates** of data download using the climetlab plugin. This has the advantage that of allowing no hallucinations from the LLM when it comes to code generation. On the downside, it also reduces the flexibility of the solution. In case other datasets require different data access methods, the \"Data Downloader\" tool has plenty of room for optimization. \n",
        "\n",
        "This notebook provides a **starting point** for LLM-powered application development. It shows that it is possible to interact with and analyze earth observation data using LLMs and is intended to be an **inspiration for more user projects** in this area. \n",
        "\n",
        "\n",
        "If you have been working on similar projects, you want to share feedback or in case of questions, please do not hesitate to contact me (annalena.erdmann@eumetsat.int). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05a4663b",
      "metadata": {},
      "source": [
        "## <a id='resources'></a> 4. References and Additional Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74af30d0",
      "metadata": {},
      "source": [
        "### 4.1  References of this notebook "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d21bcd4d",
      "metadata": {},
      "source": [
        "<a id=\"Naveedetal2023\"></a>\n",
        "Naveed, Humza, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. “A Comprehensive Overview of Large Language Models.” arXiv, April 9, 2024. http://arxiv.org/abs/2307.06435.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a9771f3",
      "metadata": {},
      "source": [
        "<a id=\"Songetal2023\"></a>\n",
        "Song, Yifan, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, et al. “RestGPT: Connecting Large Language Models with Real-World RESTful APIs.” arXiv, August 26, 2023. http://arxiv.org/abs/2306.06624.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "175981ac",
      "metadata": {},
      "source": [
        "<a id=\"Vasvanietal2017\"></a>\n",
        "Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, August 1, 2023. http://arxiv.org/abs/1706.03762.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b8c707",
      "metadata": {},
      "source": [
        "<a id=\"Yaoetal2023\"></a>\n",
        "Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. “ReAct: Synergizing Reasoning and Acting in Language Models.” arXiv, March 9, 2023. http://arxiv.org/abs/2210.03629."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb424cf6",
      "metadata": {},
      "source": [
        "<a id=\"Xietal2017\"></a>\n",
        "Xi, Zhiheng, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, et al. “The Rise and Potential of Large Language Model Based Agents: A Survey.” arXiv, September 19, 2023. http://arxiv.org/abs/2309.07864.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5b47168",
      "metadata": {},
      "source": [
        "### 4.2 Additional Resources\n",
        "\n",
        "More resources of LLM-powered applications for geospatial data:\n",
        "\n",
        "- Roberts, Jonathan, Timo Lüddecke, Sowmen Das, Kai Han, and Samuel Albanie. “GPT4GEO: How a Language Model Sees the World’s Geography.” arXiv, May 30, 2023. http://arxiv.org/abs/2306.00020.\n",
        "\n",
        "- Zhang, Yifan, Cheng Wei, Shangyou Wu, Zhengting He, and Wenhao Yu. “GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT.” arXiv, July 15, 2023. http://arxiv.org/abs/2307.07930.\n",
        "\n",
        "- Li, Zhenlong, and Huan Ning. “Autonomous GIS: The next-Generation AI-Powered GIS.” International Journal of Digital Earth 16, no. 2 (December 8, 2023): 4668–86. https://doi.org/10.1080/17538947.2023.2278895.\n",
        "\n",
        "\n",
        "Valuable Youtube tutorials on LangChain and LLM application development: \n",
        "\n",
        "- <a href=\"https://www.youtube.com/watch?v=J_0qvRt4LNk&t=3s\" title=”Langchain1”>LangChain Basics Tutorial #1 - LLMs & PromptTemplates with Colab</a>\n",
        "- <a href=\"https://www.youtube.com/watch?v=ziu87EXZVUE&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ&index=11\" title=”Langchainagents”>LangChain Agents - Joining Tools and Chains with Decisions</a>\n",
        "- <a href=\"https://www.youtube.com/watch?v=biS8G8x8DdA&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ&index=20\" title=”Langchaincustom”>Building Custom Tools and Agents with LangChain (gpt-3.5-turbo)</a>\n",
        "\n",
        "More information on WEkEO, WEkEO Data Access Tools and WEkEO datasets: \n",
        "\n",
        "- <a href=\"https://wekeo.eu/\">WEkEO Website</a>\n",
        "- <a href=\"https://help.wekeo.eu/en/articles/8490164-how-to-use-the-climetlab-wekeo-plugin\">WEkEO climetlab Plugins</a>\n",
        "- <a href=\"https://www.wekeo.eu/data?view=catalogue\">WEkEO Data Catalogue</a>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "<a href='https://www.futurelearn.com/courses/artificial-intelligence-for-earth-monitoring/1/steps/1291928' target='_blank'><< Back to FutureLearn</a><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src='./img/copernicus_logo.png' alt='Copernicus logo' align='left' width='20%'></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Course developed for <a href='https://www.eumetsat.int/' target='_blank'> EUMETSAT</a>, <a href='https://www.ecmwf.int/' target='_blank'> ECMWF</a> and <a href='https://www.mercator-ocean.fr/en/' target='_blank'> Mercator Ocean International</a> in support of the <a href='https://www.copernicus.eu/en' target='_blank'> EU's Copernicus Programme</a> and the <a href='https://wekeo.eu/' target='_blank'> WEkEO platform</a>.\n"
      ]
    }
  ],
  "metadata": {
    "author": "Anna-Lena Erdmann",
    "description": "This notebook creates a chatbot to answer questions about the Earth by analyzing EO data.",
    "image": "./img/thumb.png",
    "kernelspec": {
      "display_name": "Python 3.11.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "services": {
      "wekeo": {
        "git": {
          "link": "https://github.com/wekeo/ai4EM_MOOC/blob/main/2_ai4eo/2F_LLM_for_EO_data/LLMs_for_EO_data_analysis.ipynb",
          "service_contact": "support@wekeo.eu",
          "service_provider": "WEKEO"
        },
        "jupyter": {
          "link": "https://jupyterhub.prod.wekeo2.eu/hub/user-redirect/lab/tree/public/ML/2_ai4eo/2F_LLM_for_EO_data/LLMs_for_EO_data_analysis.ipynb",
          "service_contact": "support@wekeo.eu",
          "service_provider": "WEKEO"
        }
      }
    },
    "tags": {
      "domain": "Machine Learning",
      "platform": "Metop",
      "sensor": "AVHRR",
      "subtheme": "Large Language Models",
      "tags": "Sea Surface Temperature"
    },
    "title": "Large Language Models for EO data analysis",
    "vscode": {
      "interpreter": {
        "hash": "69a843eb2806097c81d46a99bb39f1f0d8214ba0939f50116f16a105a7e284d3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
