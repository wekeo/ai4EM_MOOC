{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='../../img/ai4eo_logos.jpg' alt='Logos AI4EO MOOC' width='80%'></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://www.futurelearn.com/courses/artificial-intelligence-for-earth-monitoring/1/steps/1280525' target='_blank'><< Back to FutureLearn</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global cloud classification with CUMULO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>by Valentina Zantendeschi, INRIA and University College London</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the video tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<div align=\"center\"><iframe src=\"https://player.vimeo.com/video/631909930?h=c149fd6c17\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen align=\"middle\"></iframe></div>')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling clouds and understanding their interactions with the climate system is one of the key problems for reducing uncertainty in future climate projections. An important first step in reducing this uncertainty is to accurately classify cloud types at high spatial and temporal resolution.\n",
    "\n",
    "In this workflow, we are going to train a Machine Learning model (`Gradient Boosting of Decision Trees`) for classifying clouds into one of the eight <a href='https://cloudatlas.wmo.int/en/clouds-genera.html' target='_blank'>World Meteorological Organization (WMO) genera</a>.\n",
    "\n",
    "This workflow makes use of <a href='https://github.com/FrontierDevelopmentLab/CUMULO' target='_blank'>CUMULO</a>, a dataset that provides **hyperspectral satellite images** at global scale and **cloud labels** for a subset of pixels (only along the track of the satellite).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow makes use of <a href='https://lightgbm.readthedocs.io/en/latest/index.html' target='_blank'>LightGBM</a>, a gradient boosting framework that uses ensembles of tree-based learning algorithms. We choose this approach as it is computationally efficient and it provides models with high accuracy.\n",
    "\n",
    "Given the small number of labels, we train a model at a pixel level and subsample data (**tiles**) in the labelled region.\n",
    "More precisely, we use the `LightGBM` framework to build a set of decision trees and combine their predictions to fit the problem. This is done by iteratively updating the current model by minimizing the chosen objective function (the average loss on the training sample). At each iteration, `LightGBM` grows decision trees leaf-wise: it chooses the leaf with the maximal loss improvement and adds children nodes to it, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://github.com/FrontierDevelopmentLab/CUMULO' target='_blank'>CUMULO</a> is a benchmark dataset for training and evaluating global cloud classification models. It merges two satellite products from <a href='https://atrain.nasa.gov/' target='_blank'>NASA's A-train constellation</a>: the <a href='https://modis.gsfc.nasa.gov/about/' target='_blank'>Moderate Resolution Imaging Spectroradiometer (MODIS)</a> onboard the Aqua satellite and the <a href='http://www.cloudsat.cira.colostate.edu/data-products/2b-cldclass-lidar' target='_blank'>2B-CLDCLASS-LIDAR product</a> derived from the combination of CloudSat Cloud Profiling Radar (CPR) and CALIPSO Cloud‐Aerosol Lidar with Orthogonal Polarization (CALIOP).\n",
    "\n",
    "* #### MODIS products\n",
    "MODIS products are hyperspectral images (**swaths**) of size 1354 x 2030 pixels saved at a spatial resolution of 1km by 1km. \n",
    "The MODIS detector measures 36 spectral bands (**channels**) between 620 and 14385 nm. Different _cloud properties_ can then be inferred from these channels (e.g. Cloud Temperature or Clouds Top Height), but no cloud labels are available. \n",
    "\n",
    "* #### 2B-CLDCLASS-LIDAR product\n",
    "2B-CLDCLASS-LIDAR provides _cloud labels_ corresponding to the eight World Meteorological Organization (WMO) genera and other useful vertical information. These annotations are derived considering known properties of the WMO cloud types and thanks to the additional instruments. Nevertheless, they are not provided everywhere, but only on pixel-width (1km x 1km) ‘tracks’ of the satellites, with a revisit period of 16 days (no global coverage at 1km resolution is available even after 16 days).\n",
    "\n",
    "<br>\n",
    "\n",
    "Here is an example of one swath: its visible band (left) and its cloud mask (right) with its overlying label mask (the colored pixels in the vertical track).\n",
    "<img src=\"figures/one-swath1.png\" width=60%>\n",
    "\n",
    "CUMULO brings these complementary datasets together, collocating CloudSat and MODIS-Aqua products. It contains over 300k annotated multispectral images at 1km by 1km resolution, providing daily full coverage of the Earth for 2008, 2009 and 2016. CUMULO data is available in netCDF.\n",
    "\n",
    "The cloud layer types are classified as follows:\n",
    "* `0 - Cirrus` \n",
    "* `1 - Altostratus`\n",
    "* `2 - Altocumulus` \n",
    "* `3 - Stratus` \n",
    "* `4 - Stratocumulus` \n",
    "* `5 - Cumulus` \n",
    "* `6 - Nimbostratus` \n",
    "* `7 - Deep Convection` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href='https://github.com/FrontierDevelopmentLab/CUMULO' target='_blank'>CUMULO Github repository</a>\n",
    "* <a href='https://arxiv.org/abs/1911.04227' target='_blank'>CUMULO: A Dataset for Learning Cloud Classification</a>\n",
    "* <a href='https://lightgbm.readthedocs.io/en/latest/index.html' target='_blank'>LightGBM - Gradient Boosting Framework by Microsoft</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook outline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1 - Data preparation](#data_preparation_6c)\n",
    "* [2 - Data loading](#data_loading_6c)\n",
    "* [3 - Define and train a LightLBM model](#model_setup_6c)\n",
    "* [4 - Predict cloud classes with the trained lightGBM model](#predict_6c)\n",
    "* [5 - Evaluate the model performance](#model_evaluation_6c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc4\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "sys.path.append(\"src/\")\n",
    "from nc_tile_extractor import extract_cloudy_labelled_tiles\n",
    "import zipfile\n",
    "import lightgbm as lgb\n",
    "from loader import CumuloDataset\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='read_nc'></a>Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc(nc_file):\n",
    "    \"\"\"return masked arrays, with masks indicating the invalid values\"\"\"\n",
    "    \n",
    "    file = nc4.Dataset(nc_file, 'r', format='NETCDF4')\n",
    "    \n",
    "    f_radiances = np.vstack([file.variables[name][:] for name in radiances])\n",
    "    f_properties = np.vstack([file.variables[name][:] for name in properties])\n",
    "    f_rois = file.variables[rois][:]\n",
    "    f_labels = file.variables[labels][:]\n",
    "    f_lats = file.variables[coordinates[0]][:]\n",
    "    f_longs = file.variables[coordinates[1]][:]\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "    return f_radiances, f_properties, f_rois, f_labels, f_lats, f_longs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='data_preparation_6c'></a>1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This workflow makes use of the <a href='https://www.dropbox.com/sh/6gca7f0mb3b0ikz/AADq2lk4u7k961Qa31FwIDEpa?dl=0' target='_blank'>CUMULO</a> benchmark dataset. A subset of the benchmark dataset required for this workflow is already available in the folder `./DATA/nc/`. If you want to reproduce the example on your local machine, you can download and the data subset with the following command:\n",
    "> curl -L -o month_1_day_18.zip https://www.dropbox.com/sh/6gca7f0mb3b0ikz/AABdbAzfbcCpwEPHbp9zBqWKa/CUMULO/2008/01/018/daylight?dl=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a following step, you can unzip the downloaded `.zip` folder with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"./DATA/nc/month_1_day_18.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"/home/jovyan/DATA/nc/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subset of the CUMULO benchmark dataset consists of 150 files in the `NetCDF` format, following the <a href='http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html' target='_blank'>CF convention</a>.\n",
    "\n",
    "There is one NetCDF per swath, one every 5 minutes and at a spatial resolution of 1km by 1km:\n",
    "\n",
    "* **filename** =  AYYYYDDD.HHMM.nc\n",
    "          \n",
    "with \n",
    "* `YYYY` = year\n",
    "* `DDD` = absolute day since 01.01.2008 \n",
    "* `HH` = hour of day\n",
    "* `MM` = minutes\n",
    "\n",
    "The data files are valid for 18 January 2008. Let us open one file and inspect its content. You can open a `netCDF` file with the `Dataset()` constructor of the `netCDF4` library. You can see that each file contains 31 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_dir = \"/home/jovyan/DATA/nc/\"\n",
    "nc_files = glob.glob(nc_dir+\"*.nc\")\n",
    "file = nc4.Dataset(nc_files[0], 'r', format='NETCDF4')\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUMULO provides different types of features (**channels**) per georeferenced grid-point. We make use of:\n",
    "* `13 calibrated radiances` as input channels, as they capture the physical properties needed for cloud classification\n",
    "* the available `labels` as groundtruth (one label per tile)\n",
    "* the `cloud_mask` to select which grid-points to analyze (only the cloudy ones) \n",
    "* `cloud properties` for physically validating our model. These channels are derived from the raw radiances and provide higher level properties of a cloud, such as its top pressure, temperature, etc.\n",
    "\n",
    "Here is the list of variables we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = ['latitude', 'longitude']\n",
    "radiances = ['ev_250_aggr1km_refsb_1', 'ev_250_aggr1km_refsb_2', 'ev_1km_emissive_29', 'ev_1km_emissive_33', 'ev_1km_emissive_34', 'ev_1km_emissive_35', 'ev_1km_emissive_36', 'ev_1km_refsb_26', 'ev_1km_emissive_27', 'ev_1km_emissive_20', 'ev_1km_emissive_21', 'ev_1km_emissive_22', 'ev_1km_emissive_23']\n",
    "properties = ['cloud_water_path', 'cloud_optical_thickness', 'cloud_effective_radius', 'cloud_phase_optical_properties', 'cloud_top_pressure', 'cloud_top_height', 'cloud_top_temperature', 'cloud_emissivity', 'surface_temperature']\n",
    "rois = 'cloud_mask'\n",
    "labels = 'cloud_layer_type'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to extract from each file tiles with a `3x3 pixel window size`. These tiles will be used as input to the Machine Learning model. The `3x3 pixel tiles` are sampled around each labelled pixel of an image. Thus, tiles might overlap. The tiles shall be stored in the `.npz` format, which is a zipped archive of files, of which each contains one variable in the `.npy` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_dir = Path(\"/home/jovyan/DATA/nc/\")\n",
    "\n",
    "# make directory where tiles will be stored\n",
    "save_dir = Path(\"/home/jovyan/DATA/npz/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# retrieve all files in netcdf format\n",
    "nc_paths = nc_dir.glob(\"*.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us go over each netCDF file, and execute the following operations:\n",
    "1. extract the required variables from the netCDF file with the helper function [read_nc()](#read_nc). The function loads a netCDF file and returns four masked arrays with the required `radiances`, `properties`, `cloud_mask` and `cloud_labels`.\n",
    "2. apply the function `extract_cloudy_labelled_tiles`, which extracts all tiles from a cloud area that are labelled. \n",
    "3. save the tile-based `radiances`, `properties`, `cloud_mask`, `cloud_labels` and `location` as uncompressed `.npz` file\n",
    "\n",
    "The `.npz` files are stored under `./DATA/npz/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(nc_paths):\n",
    "    \n",
    "    # load swath variables and label masks\n",
    "    f_radiances, f_properties, f_cloud_mask, f_labels, *_ = read_nc(filename)\n",
    "    \n",
    "    # labelled pixels have at least one non-zero value over the vertical axis\n",
    "    f_label_mask = np.sum(~f_labels.mask, 3) > 0\n",
    "    \n",
    "    # for the purposes of this tutorial, we are going to extract only labelled tiles\n",
    "    try:\n",
    "        labelled_tiles, labelled_positions = extract_cloudy_labelled_tiles((f_radiances, f_properties, f_cloud_mask, f_labels), f_cloud_mask[0], f_label_mask[0])\n",
    "        \n",
    "        name = os.path.basename(filename).replace(\".nc\", \".npz\")\n",
    "\n",
    "        np.savez_compressed(save_dir / name, \n",
    "                            radiances=labelled_tiles[0].data, \n",
    "                            properties=labelled_tiles[1].data, \n",
    "                            cloud_mask=labelled_tiles[2].data, \n",
    "                            labels=labelled_tiles[3].data, \n",
    "                            location=labelled_positions)\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='data_loading_6c'></a>2. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load the data for learning the ML cloud classifier. `CumuloDataset()` is a constructor which loads the tiles that we have just extracted. When called, it returns the radiance channels that we use as inputs and the labels that we use as outputs for the `LightLBM` model.\n",
    "\n",
    "As one tile can have multiple labels, `CumuloDataset()` returns only the most frequent one.\n",
    "It requires as arguments:\n",
    "* `root_dir`: the directory where the tiles are stored \n",
    "* `ext`: the extension/format of the files containing the tiles that are going to be read (npz in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CumuloDataset(root_dir='/home/jovyan/DATA/npz/', ext=\"npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we want to load all tiles at once and vectorize them, as required by the `LightGBM` model. The result are the following vectorized tiles:\n",
    "* `X` - Radiances as input for the model\n",
    "* `y` - cloud labels as output for the model\n",
    "* `p` - Physical properties which are used later to evaluate the outcomes also on a physical level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, props = [], [], []\n",
    "\n",
    "for filename, radiances, properties, cloud_mask, labels in dataset:\n",
    "    xs.append(radiances) # radiances\n",
    "    ys.append(labels) # labels\n",
    "    props.append(properties) # we load also the physical properties and use them later on for physical evaluation\n",
    "\n",
    "shape = xs[0].shape\n",
    "X = np.vstack(xs).reshape(-1, shape[1] * shape[2] * shape[3]) # vectorize tiles\n",
    "y = np.hstack(ys)\n",
    "\n",
    "shape = props[0].shape\n",
    "p = np.vstack(props).reshape(-1, shape[1] * shape[2] * shape[3]) # vectorize tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, 63945 tiles were loaded and the shapes of the `input`, `output` and `properties` are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can split the input and output variables into subsets for `training`, `validation` and `testing`:\n",
    "* `training data`: is the actual sample used to train the Machine Learning model\n",
    "* `validation data`: used to evaluate a model during training, but the model does not learn from this data\n",
    "* `test data`: used to provide an evaluation of the final model and it is only used when a model is completely trained\n",
    "\n",
    "It is important to define such sets in order to have a correct evaluation of the model performance and to ensure that the model not just simply memorizes its training data.\n",
    "\n",
    "<a href='https://scikit-learn.org/stable/' target='_blank'>scikit-learn</a> offers a function called `train_test_split()` which congruently splits the arrays into two.\n",
    "\n",
    "The function takes the following keyword arguments (kwargs):\n",
    "* `arrays`: input, output data arrays\n",
    "* `test_size`: a float number representing the proportion of the dataset to include in the test subset\n",
    "* `random_state`: An integer assuring reproducibility of the random shuffling of the data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split resulted in the following subsets: `training: 46040 tiles`, `validation: 5116 tiles` and `testing: 12789 tiles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 20% of data as test set\n",
    "train_xs, test_xs, train_ys, test_ys = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# we use 10% of the remaining data for validation\n",
    "train_xs, val_xs, train_ys, val_ys = train_test_split(train_xs, train_ys, test_size=0.10, random_state=42)\n",
    "\n",
    "print(train_xs.shape, train_ys.shape, val_xs.shape, val_ys.shape, test_xs.shape, test_ys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='model_setup_6c'></a>3. Define and train a LightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load the `lightGBM` library and to convert the `training` and `validation` subsets into a `lightgbm.basic.Dataset`. You can do the conversion with the `Dataset` constructor of the `lightgbm` library. The constructor requires input and output subsets for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(train_xs, train_ys)\n",
    "lgb_valid = lgb.Dataset(val_xs, val_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify a set of `hyperparameters`, with which a Machine Learning model can be fine-tuned. LightGBM offers several algorithms and the hyperparameters strongly affect the performance of the model.\n",
    "\n",
    "Let us set the following parameters for the training process:\n",
    "\n",
    "* `boosting_type`: *gbdt* for traditional Gradient Boosting Decision Tree (other algorithms are available too)\n",
    "* `objective`: *multiclass* means we optimize a softmax-based loss over the classes\n",
    "* `num_classes`: *8* as the number of cloud types we want to identify is eight\n",
    "* `num_iterations`: *400* maximal number of iterations, where at each iteration a new decision tree per class is fit onto the problem\n",
    "* `num_leaves`: maximal number of leaves of a tree\n",
    "* `learning_rate`: *0.1* the step value for model update\n",
    "* `verbose`: we set the verbosity level to *0* to print only errors and warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_classes': 8,\n",
    "    'num_iterations': 400,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'verbose': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to run the algorithm on the training data subset, based on the hyperparameters defined above. You can use the function `train()` from the `lightGBM` library. The functions takes the following keyword arguments:\n",
    "* `params`: defined hyperparameters\n",
    "* `dataset`: training data subset\n",
    "* `valid_sets`: validation data subset\n",
    "\n",
    "The validation dataset is used in order to select the best performing model based on the performance of this subset. \n",
    "Indeed, the best model is not necessarily the one obtained at the end of the training cycle, as it might have started to memorize the training set which would result in a bad performance during the inference/testing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbm = lgb.train(params, lgb_train, valid_sets=[lgb_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='predict_6c'></a> 4. Predict cloud classes with the trained lightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see that at about iteration 380 the model **overfits**. This means that the model starts to memorize the training set and its performance on unseen data (e.g. validation data) decreases. The model with the highest performance to make cloud class predictions is the one where the `validation loss` is lowest. The best model can be identified with `gbm.best_iteration`. \n",
    "\n",
    "\n",
    "Hence, you want to use the best model to make predictions on the `training`, `validation` and `test` data subsets. At a later stage, you can then evaluate the model's performance to predict the respective data subsets.\n",
    "\n",
    "The function `gbm.predict()` allows you to predict cloud types based on the trained `lightGBM` model and the training, validation and test input data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prob_pred = gbm.predict(train_xs, num_iteration = gbm.best_iteration)\n",
    "val_prob_pred = gbm.predict(val_xs, num_iteration = gbm.best_iteration)\n",
    "test_prob_pred = gbm.predict(test_xs, num_iteration = gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prob_pred.shape, val_prob_pred.shape, test_prob_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction process returns for each input tile a vector indicating the probability of each of the eight cloud classes. The predicted cloud class is hence the cloud type/class with the highest probability. You can select the maximum argument of a numpy array with the function `argmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_pred = np.argmax(train_prob_pred, 1)\n",
    "val_y_pred = np.argmax(val_prob_pred, 1)\n",
    "test_y_pred = np.argmax(test_prob_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_pred.shape, val_y_pred.shape, test_y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='model_evaluation_6c'></a>5. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important step after the prediction process is to evaluate the predicted cloud classes. Let us start with creating and analysing the `confusion matrices` for the predictions based on `training`, `validation` and `test` input data. A confusion matrix is a common table layout that allows the visualisation of the performance of an algorithm.\n",
    "\n",
    "Each component `c[i,j]` of a confusion matrix is equal to the ratio of tiles of class `i` that have been classified as class `j`.\n",
    "This means that on the diagonal of the confusion matrix, we have the ratio of tiles that have been correctly classified, while on the rest of the matrix we see how often tiles of a target class are misclassified as belonging to another particular class. \n",
    "\n",
    "The built-in sklearn's function `confusion_matrix()` can be called to compute a confusion matrix, providing the following arguments:\n",
    "- a vector of groundtruth classes\n",
    "- a vector of the same size of predicted classes\n",
    "- `labels`: the classes for which the scores are computed\n",
    "- `normalize='true'` to return ratios instead of counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cm = confusion_matrix(train_ys, train_y_pred, labels=range(8), normalize='true')\n",
    "val_cm = confusion_matrix(val_ys, val_y_pred, labels=range(8), normalize='true')\n",
    "test_cm = confusion_matrix(test_ys, test_y_pred, labels=range(8), normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to visualize the three confusion matrices as `heatmap`. You can use the function `heatmap()` from the seaborn library to visualise a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "\n",
    "for i, (label, cm) in enumerate(zip([\"TRAINING\", \"VALIDATION\", \"TEST\"], [train_cm, val_cm, test_cm])):\n",
    "    plt.subplot(131+i)\n",
    "\n",
    "    df_cm = pd.DataFrame(cm, index=range(8), columns=range(8))\n",
    "\n",
    "    plt.title(label)\n",
    "    ax = sn.heatmap(df_cm, annot=True, vmin=0, vmax=1)\n",
    "    ax.set(xlabel='predicted', ylabel='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrices above show that the `LightGBM` was able to almost perfectly fit the training data. However on the validation and test data subsets, the predictability of the model is lower. In particular, the model shows good performance on class `0 - Cirrus`, while tiles of class `7 - Deep Convection` are often missclassified as class `0 - Cirrus` or `1 - Altostratus`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, you can evaluate the perfomance of the model using two metrics that are standard for assessing the quality of Machine Learning classifiers:\n",
    "- `accuracy`: the general rate of correctly classified tiles\n",
    "- average `F1 score`: a metric that accounts for `model precision` (the ratio of true positives over all positives) and `recall` (the ratio of true positives over all tiles of that class) for each cloud type\n",
    "\n",
    "You can use the the functions `accuracy_score()` and `f1_score()` from the scikit-learn library to calculate the two metrics. `F1 Score` can range from [0, 1], which 1 indicating perfect precision and recall. Thus, the higher the `F1 score` the better.\n",
    "\n",
    "The accuracy score is around 0.84 and indicates that the model is 84% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(test_ys, test_y_pred)\n",
    "f1 = f1_score(test_ys, test_y_pred, average='macro')\n",
    "\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "print(\"Test f1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://www.futurelearn.com/courses/artificial-intelligence-for-earth-monitoring/1/steps/1280525' target='_blank'><< Back to FutureLearn</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../img/copernicus_logo.png' alt='Copernicus logo' align='left' width='20%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course developed for <a href='https://www.eumetsat.int/' target='_blank'> EUMETSAT</a>, <a href='https://www.ecmwf.int/' target='_blank'> ECMWF</a> and <a href='https://www.mercator-ocean.fr/en/' target='_blank'> Mercator Ocean International</a> in support of the <a href='https://www.copernicus.eu/en' target='_blank'> EU's Copernicus Programme</a> and the <a href='https://wekeo.eu/' target='_blank'> WEkEO platform</a>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "title": "Global cloud classification with CUMULO",
  "description": "In this workflow, you are going to train a Machine Learning model (Gradient Boosting of Decision Trees) for classifying clouds into one of the eight World Meteorological Organisation (WMO) cloud genera.",
  "author": "Valentina Zantendeschi",
  "image": "./img/img_01.png",
  "services": {
       "wekeo": {
           "git": {
              "link": "https://github.com/wekeo/ai4EM_MOOC/blob/main/6_climate/6C_cumulo_cloud_classification/6C_cumulo_cloud_classification.ipynb",
              "service_contact": "support@wekeo.eu",
              "service_provider": "WEKEO"},
          "jupyter": {
              "link": "https://jupyterhub-wekeo.apps.eumetsat.dpi.wekeo.eu/hub/user-redirect/lab/tree/public/ML/6_climate/6C_cumulo_cloud_classification/6C_cumulo_cloud_classification.ipynb",
              "service_contact": "support@wekeo.eu",
              "service_provider": "WEKEO"}
      }
 },
  "tags": {
    "domain": "Machine Learning",
    "subtheme": "Atmosphere",
    "tags": ["Cloud type", "Top-of-atmosphere radiance", "CUMULO"]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
