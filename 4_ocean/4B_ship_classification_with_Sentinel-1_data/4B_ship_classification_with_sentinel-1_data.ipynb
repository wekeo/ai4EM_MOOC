{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='../../img/ai4eo_logos.jpg' alt='Logos AI4EO MOOC' width='80%'></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.futurelearn.com/courses/artificial-intelligence-for-earth-monitoring/1/steps/1280518\"><< Back to FutureLearn</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of a VGG16 Convolutional Neural Network in order to classify ships based on Sentinel-1 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>by Leonardo De Laurentiis, University of Tor Vergata, Rome, Italy</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the video tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\"><iframe src=\"https://player.vimeo.com/video/631902398?h=27546f33ba\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen align=\"middle\"></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<div align=\"center\"><iframe src=\"https://player.vimeo.com/video/631902398?h=27546f33ba\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen align=\"middle\"></iframe></div>')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows you how you can develop and train a Convolutional Neural Network (CNN) in order to classify ships extracted from Sentinel-1 data. A VGG16 Convolutional Neural Network model is trained with the help of the [OpenSARShip dataset](https://ieeexplore.ieee.org/document/8067489), which is a benchmark dataset to develop applicable and adaptive ship interpretation algorithms. The workflow makes use of a subset of the OpenSARShip dataset, using 2805 Sentinel-1 images each labelled as one of three ship classes:\n",
    "* `0 - Bulk Carrier`\n",
    "* `1 - Container Ship`\n",
    "* `2 - Tanker`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow develops and trains a VGG16 Convolutional Neural Network (ConvNet / CNN) model, which is a Deep Learning algorithm used to classify and detect features in an image. The model was proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper ['Deep Convolutional Networks for Large-Scale Image Recognition'](https://arxiv.org/abs/1409.1556). It is one of the simpler convolutional neural networks and makes only use of basic convolutions and pooling operations, but not network information, such as residual connections.\n",
    "The number '16' stands for the layer depth of the network.\n",
    "\n",
    "**Key Features of VGG16:**\n",
    "* it is also called the `OxfordNet` model, named after the Visual Geometry Group from Oxford\n",
    "* Number 16 refers to the fact that it has a total of 16 layers that have some weights\n",
    "* uses always a 3x3 kernel for convolution\n",
    "* it only has Conv and pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow makes use of the data file [MOOC_dataset.npz](./MOOC_dataset.npz). The data file is a [.NPZ file](https://numpy.org/doc/stable/reference/generated/numpy.savez.html) which is a zipped archive of [.NPY files](https://numpy.org/doc/stable/reference/generated/numpy.lib.format.html#module-numpy.lib.format) and often used for Machine Learning applications, especially when multiple large images (numpy arrays) need to be saved in a compressed format.\n",
    "\n",
    "The data file contains 2805 images with the size `128x128` pixels. Each image consists of an extracted ship from a Sentinel-1 image with VV-polarisation and each ship has an associated ship class label, indicating whether the ship in the image is a `0 - Bulk Carrier`, a `1 - Container Ship` or a `2 - Tanker`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [OpenSARShip: A Dataset Dedicated to Sentinel-1 Ship Interpretation](https://ieeexplore.ieee.org/document/8067489)\n",
    "* [Merchant Vessel Ship Classification Based Scattering Component Analysis](https://ieeexplore.ieee.org/abstract/document/6451119)\n",
    "* [Comprehensive guide to convolutional neural networks](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "* [Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook outline\n",
    "* [1 - Inspect the labelled training data](#inspect)\n",
    "* [2 - Instantiate the VGG16 model](#instantiate_vgg16)\n",
    "* [3 - Instantiate a Keras model from input and output](#instantiate_keras)\n",
    "* [4 - Configure the Keras model for training](#configure_training)\n",
    "* [5 - Create training and test subsets from the data](#training_split)\n",
    "* [6 - Optional: Train the model](#model_train)\n",
    "* [7 - Load and evaluate the trained model](#load_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN S3FS IMPORT SNIPPET ##\n",
    "import os, sys\n",
    "s3_home =  os.getcwd()\n",
    "try: sys.path.remove(s3_home) # REMOVE THE S3 ROOT FROM THE $PATH\n",
    "except Exception: pass\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "os.chdir('/home/jovyan') # TEMPORARILY MOVE TO ANOTHER DIRECTORY\n",
    "\n",
    "# BEGIN IMPORTS #\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# END PYTHON IMPORTS #\n",
    "os.chdir(current_dir) # GO BACK TO YOUR PREVIOUS DIRECTORY\n",
    "\n",
    "sys.path.append(s3_home) # RESTORE THE S3 ROOT IN THE $PATH\n",
    "\n",
    "## END S3FS IMPORT SNIPPET ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='inspect'></a>1. Inspect the labelled training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to open the data file with the labelled training data to get a better understanding of the input data used for training the model. A `.npz` file can be opened with the `numpy.load()` function, which opens arrays stored in `.npy` and `.npz` files. The function returns a `NpzFile` object. With the function `.files()`, you can see an overview of the files that are part of the `.npz` file. \n",
    "\n",
    "You see that a set of two items are available: `image` and `label`.\n",
    "\n",
    "<i>**Note:** Sentinel-1 images for this example only contain VV-polarisation</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image', 'label']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('./MOOC_dataset.npz') \n",
    "files = data.files \n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, you can inspect the shape of the two items. The item `image` has a shape of [2805, 128, 128] and the item `label` is a one-dimensional array consisting of 2805 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape - \"Image\" item: (2805, 128, 128)\n",
      "Shape - \"Label\" item: (2805,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape - \"Image\" item: ' + str(data['image'].shape))\n",
    "print('Shape - \"Label\" item: ' + str(data['label'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualize one image with the function `plt.imshow()` and print the associated ship class from the `label` item. You can change the index number to see other image examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ship class: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAASo0lEQVR4nO3dbYxcV33H8e9vZvbBj7E3iYNjhzpIFhDSQtIVJNBWESYQ0ginlSIZNZVVIlmt0hIQErXLi6gvkCIVIXhRqCyerBIlikzaWBGBGANCVCJkIRTsOI5dQmMnxg41ebLj3Xn498XctcfrWe/u3Lkzuz6/j7SamXtn5v7t3fObc8/ce48iAjNLV6nfBZhZfzkEzBLnEDBLnEPALHEOAbPEOQTMEldYCEi6RdIBSYckbS1qO2aWj4o4TkBSGXgWuBk4AjwJfDQinu76xswsl0pB7/tu4FBE/BpA0oPARqBtCAxqKIZZUlApZgbwGr//XURcPnV5USGwBjjc8vgI8J7WJ0jaAmwBGGYx79GGgkoxM4Dvxc7/bbe8qDEBtVl2zn5HRGyPiNGIGB1gqKAyzGwmRYXAEeCqlsdrgRcL2paZ5VBUCDwJrJd0taRBYBOwq6BtmVkOhYwJRERN0t8D3wXKwNciYl8R2zKzfIoaGCQivg18u6j3N7Pu8BGDZolzCJglziFgljiHgFniHAJmiXMImCXOIWCWOIeAWeIcAmaJcwiYJc4hYJY4h4BZ4hwCZolzCJglziFgljiHgFniHAJmiXMImCXOIWCWOIeAWeIcAmaJcwiYJc4hYJY4h4BZ4hwCZonrOAQkXSXpB5L2S9on6Z5s+Yik3ZIOZrcru1eumXVbnp5ADfhURLwduAG4W9I1wFZgT0SsB/Zkj81snuo4BCLiaET8PLv/GrAfWANsBHZkT9sB3J6zRjMrUFfGBCStA64DngCuiIij0AwKYNU0r9kiaUzSWJXxbpRhZh3IHQKSlgLfAj4REa/O9nURsT0iRiNidIChvGWYWYdyhYCkAZoBcH9EPJwtPiZpdbZ+NXA8X4lmVqQ83w4I+CqwPyI+37JqF7A5u78ZeKTz8sysaJUcr30f8NfAryT9Ilv2T8B9wEOS7gKeB+7IVaGZFarjEIiIHwOaZvWGTt/XzHrLRwyaJc4hYJY4h4BZ4hwCZolzCJglziFgljiHgFniHAJmiXMImCXOIWCWOIeAWeIcAmaJcwiYJc4hYJY4h4BZ4hwCZolzCJglziFgljiHgFniHAJmiXMImCXOIWCWOIeAWeIcAmaJcwiYJa4bsxKXJT0l6dHs8Yik3ZIOZrcr85dpZkXpRk/gHmB/y+OtwJ6IWA/syR6b2TyVd2rytcCfA19pWbwR2JHd3wHcnmcbZlasvD2BLwCfBhoty66IiKMA2e2qdi+UtEXSmKSxKuM5yzCzTnUcApJuA45HxM86eX1EbI+I0YgYHWCo0zLMLKeOpyYH3gd8RNKtwDCwXNI3gWOSVkfEUUmrgePdKNTMitFxTyAitkXE2ohYB2wCvh8RdwK7gM3Z0zYDj+Su0swKU8RxAvcBN0s6CNycPTazeSrP7sAZEfFD4IfZ/f8DNnTjfc2seD5i0CxxDgGzxDkEzBLnELAmqd8VnGu+1XMRcwjY2QZXRMOTOntflRwEPeIQuJh02mgiskY35c9hsgGXyvneO2Lur2vUp3+dw6GrHAIXk04aG+RrVO1e60a6oDgEUied7QFEo/1zonGBT+U23XaVmr2HUrk79bXbpnVNVw4WsgUsgnNPAp3uOdOtaxMQ04VJJ9ptu5vvbw4Bo31Dm22Xvm0j7XC3ZLaKfv/EuF9l7U03WGgXHf+GbVoqCZU6/Ipvxjf34OF84RCwC5vsDXS70c61S+/QKIxD4GJQ2EE+M3xr0CtFHsxkDoEFr1cNo9+DcR6bKIy/HVjoOmmc0syviyBq1c620RpM3QiPCIh6/vexthyvNr1OD/mdLXfv5wX3BFLU6+/xp+sZTN6fTc/ECuOegBVvsoG7oc9LDgFrb/IMwk677FNf24+Tm2xWvDtg7eX91L7QacDnnWtwoXMT3HsomnsC1nt5ehjWde4J2PS6/VVf3veZrMe9g65yT8AuzAfpXPT8G7YLu9AFRaC33fqij1tIVK4QkLRC0k5Jz0jaL+lGSSOSdks6mN2u7Fax1gWzabRnrit4gVOJvV9/0cjbE/gi8J2IeBvwTmA/sBXYExHrgT3ZY5sPZhsAKqFy+eypxOetbz25yJ/MC13HISBpOfBnwFcBImIiIl4GNgI7sqftAG7PV6J1zWy60xGUBgcoLV9K6ZLllJYuObtu8uu9iJmvBpz3OAPrmTw9gbcALwFfl/SUpK9IWgJcERFHAbLbVe1eLGmLpDFJY1XGc5RhXTcwgAYHoVKBcvn8T/+p3OgXtDwhUAGuB74cEdcBJ5lD1z8itkfEaESMDjCUowzrGglVKpSWLCZWLEMS1OtnxgbOucpQS6NXuYwqA+fuIrT+2LyW5ziBI8CRiHgie7yTZggck7Q6Io5KWg0cz1ukFexMwy6hwUFO/vGbOf6uAaJyOYMvw5pvPkOcHm826GoNokE04szFRqIRqOQrAC9UHYdARPxW0mFJb42IA8AG4OnsZzNwX3b7SFcqtWK0BsBABQ0NcfSGCn/3l9/m1qX72PnK9fz4sWvR719tNvrxCahWoVYjGqXm2AANn+6/gOU9YvAfgPslDQK/Bv6G5i7GQ5LuAp4H7si5DevEbE/PPXM6L1CvExMTDJ8Qj/32Wm56ywEuqZyisWyY8qnTxKuvQSP79M92E1SpNHsF57ynE2EhyRUCEfELYLTNqg153te6ZC7n6UcDKBO1GkMngoMvrOKHb3orz55605neQtSzxl1qGSMolxH184OgW/X5UOHC+dyBi9VcG00EUatBvc5lu57h8h8t5zuX/SkApedeoDFRJSaqlJYsgoFByouGoVqj8cbp7LiCLCTmclHSufRUrDAOATtXBI3XT6LxcUqvn4KSiDdOn9/AGwHlMqUli4hqjajVUKg5TuDdgQXFIWDniXodSoKTJwFonDrVXCE1G3+jOXag4SFYOYJeeR1OniQg2zVoeTN35+c9h4C1V6+DRExpvI3xcVSvo0XDaOkS6iuXUiKbrQiaA4VLF6PT48Sp08TJk82eQnWiL/8Mm5lDIBVzHCSMRgnqU/bvI4hqDRpBafkyYmiQ+nAFLR6i1Agai4dpDFeYWDHEwOtVyidOIoDTp5uXL3dvYF5yCKRiLg0wm648alNCQKK0ZDEaHiZGLoFqjYFnX6C6/kpe+8MVLPvYCyypTPDsf69j8YtDLD2ymKGXL2XwtSqVJw8QExNnv2GIODeYiriAic2Krydg7U097DdrpJJQuURj8SCxaBCVStQXV6guFdePHObaS15EDUFASDQGRKM8h7kMHQA9557AxaCHg28RgYDxkSHQMJUVw7xxaYXasPjVy1fyu1NLeNN/BQMnq1TeqFOaaFA6VSVqtewbhjbzDvSodmvPIXAx6EUDyrrvcXqcBrDo8KtEpQQNWFZtMHxigMOPraN8Gi49cAIaQAlUa6DTE9Qb0TzByF8fzjsOAZu9CKI6QVQnKP2mjioVWDRM5VhQAd68d4Co1agff4nysmVo2dLmoci1Wr8rtwtwCNjcSc1BvokJeON0c1k0mtceaDTHERpvnEYtg4BzPprQesYhYHMz9TyCaPmUb/nEj3odJg8NiIYDYB5zCNjczObKw9nlx6Lh/f+FwCFg3eMR/gXJxwmYJc4hYJY4h4BZ4hwC1j1zvey4L08+LzgErHsmBwY9QLigOASss0/k6T7x53y2ovWbvyK0zhqjG/BFwz0Bs8Q5BMwS5xAwS5xDwCxxuUJA0icl7ZO0V9IDkoYljUjaLelgdruyW8WaWfd1HAKS1gAfB0Yj4lqgDGyiOTPxnohYD+xhDtOVm1nv5d0dqACLJFWAxcCLwEZgR7Z+B3B7zm2YWYE6DoGIeAH4HM2Zh48Cr0TE48AVEXE0e85RYFW710vaImlM0liV8U7LMLOc8uwOrKT5qX81cCWwRNKds319RGyPiNGIGB1gqNMybCGby3kGVpg8uwMfAJ6LiJciogo8DLwXOCZpNUB2ezx/mdYzvWyUrfMaWN/kCYHngRskLZYkYAOwH9gFbM6esxl4JF+J1jOTAeBP56R0fO5ARDwhaSfwc6AGPAVsB5YCD0m6i2ZQ3NGNQq0H/KmcpFwnEEXEvcC9UxaP0+wVmNkC4CMGzRLnEDBLnEPALHEOAbPEOQTMEucQMEucQ8AscQ4Bs8Q5BMwS5xAwS5xDwCxxDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEucQ8AscQ4Bs8Q5BMwS5xAwS5xDwCxxDgGzxDkEzBI3YwhI+pqk45L2tiwbkbRb0sHsdmXLum2SDkk6IOlDRRVuZt0xm57AN4BbpizbCuyJiPXAnuwxkq4BNgHvyF7zJUnlrlVrZl03YwhExI+AE1MWbwR2ZPd3ALe3LH8wIsYj4jngEPDu7pRqZkXodEzgiog4CpDdrsqWrwEOtzzvSLbsPJK2SBqTNFZlvMMyzCyvbg8MtpvYvu181xGxPSJGI2J0gKEul2Fms9VpCByTtBoguz2eLT8CXNXyvLXAi52XZ2ZF6zQEdgGbs/ubgUdalm+SNCTpamA98NN8JZpZkSozPUHSA8BNwGWSjgD3AvcBD0m6C3geuAMgIvZJegh4GqgBd0dEvaDazawLZgyBiPjoNKs2TPP8zwKfzVOUmfWOjxg0S5xDwCxxDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEucQ8AscQ4Bs8Q5BMwS5xAwS5xDwCxxDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEucQ8AscQ4Bs8Q5BMwS5xAwS9yMISDpa5KOS9rbsuxfJD0j6ZeS/kPSipZ12yQdknRA0ocKqtvMumQ2PYFvALdMWbYbuDYi/gh4FtgGIOkaYBPwjuw1X5JU7lq1ZtZ1M4ZARPwIODFl2eMRUcse/oTmFOQAG4EHI2I8Ip4DDgHv7mK9ZtZl3RgT+BjwWHZ/DXC4Zd2RbNl5JG2RNCZprMp4F8ows07kCgFJn6E5Bfn9k4vaPC3avTYitkfEaESMDjCUpwwzy2HGqcmnI2kzcBuwISImG/oR4KqWp60FXuy8PDMrWkc9AUm3AP8IfCQiTrWs2gVskjQk6WpgPfDT/GWaWVFm7AlIegC4CbhM0hHgXprfBgwBuyUB/CQi/jYi9kl6CHia5m7C3RFRL6p4M8tPZ3vy/bNcI/Eebeh3GWYXte/Fzp9FxOjU5T5i0CxxDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEjcvjhOQ9BJwEvhdv2sBLsN1tHId51rIdfxBRFw+deG8CAEASWPtDmRwHa7DdRRbh3cHzBLnEDBL3HwKge39LiDjOs7lOs510dUxb8YEzKw/5lNPwMz6wCFglrh5EQKSbsnmKTgkaWsPt3uVpB9I2i9pn6R7suUjknZLOpjdruxBLWVJT0l6tI81rJC0M5tTYr+kG/tUxyez38deSQ9IGu5VHdPMszHttouaZ6OX8330PQSyeQn+FfgwcA3w0Wz+gl6oAZ+KiLcDNwB3Z9veCuyJiPXAnuxx0e4B9rc87kcNXwS+ExFvA96Z1dPTOiStAT4OjEbEtUCZ5lwWvarjG5w/z0bbbRc8z0a7OoqZ7yMi+voD3Ah8t+XxNmBbn2p5BLgZOACszpatBg4UvN21NP+43g88mi3rdQ3LgefIBotblve6jsnL1o/QvPzdo8AHe1kHsA7YO9P/wdS/VeC7wI1F1TFl3V8A93ejjr73BJjDXAVFkrQOuA54ArgiIo4CZLerCt78F4BPA42WZb2u4S3AS8DXs92Sr0ha0us6IuIF4HPA88BR4JWIeLzXdUwx3bb7+bfb0Xwf7cyHEJj1XAWFFSAtBb4FfCIiXu3xtm8DjkfEz3q53TYqwPXAlyPiOprncvRsfGZStr+9EbgauBJYIunOXtcxS335280z30c78yEE+jpXgaQBmgFwf0Q8nC0+Jml1tn41cLzAEt4HfETSb4AHgfdL+maPa4Dm7+FIRDyRPd5JMxR6XccHgOci4qWIqAIPA+/tQx2tptt2z/92W+b7+KvI+v5565gPIfAksF7S1ZIGaQ5w7OrFhtW8XvpXgf0R8fmWVbuAzdn9zTTHCgoREdsiYm1ErKP5b/9+RNzZyxqyOn4LHJb01mzRBpqXju9pHTR3A26QtDj7/WygOUDZ6zpaTbftns6zUdh8H0UO8sxhAORWmqOd/wN8pofb/ROa3aZfAr/Ifm4FLqU5UHcwux3pUT03cXZgsOc1AO8CxrL/j/8EVvapjn8GngH2Av9Oc46LntQBPEBzLKJK8xP2rgttG/hM9nd7APhwwXUcornvP/m3+m/dqMOHDZslbj7sDphZHzkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEvc/wO9bMoKT3L5EgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 2800\n",
    "plt.imshow(data['image'][index,:,:])\n",
    "print('Ship class: ' + str(data['label'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step in inspecting the labelled training data, you can define the `image` item as input (`X`) and the `label` item as output (`y`) variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['image']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='instantiate_vgg16'></a>2. Instantiate the VGG16 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to instantiate a Keras tensor that is used as an entry point for the network. You can use the `layers.Input` class from Python's Tensorflow Keras API `tf.keras`. Let us define a tensor with the size `128x128x1`. The size of the tensor relates to the size of the images used for training the model.\n",
    "\n",
    "The output is a `KerasTensor` object called `inputs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 128, 128, 1) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(128, 128, 1))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to instantiate the VGG16 model. The tensorflow Keras API offers under `tensorflow.keras.applications` a set of models (canned architectures) with pre-trained weights and `VGG16` is one of them. You can instantiate a VGG16 model with the following keyword arguments:\n",
    "\n",
    "* `include_top=False` - we do not want to include the three fully-connected layers at the top of the network\n",
    "* `weights=None` - by setting weights to None, we choose random initialization\n",
    "* `input_tensor=inputs` - The input tensor is the inputs tensor created in the previous step\n",
    "* `pooling='avg'` - global average pooling will be applied to the output of the last convolutional block and this means that the output of the model will be a two-dimensional tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:**<br>\n",
    "> `Fully connected layers`: By default, the algorithm would include three fully connected layers on top, but for training purposes, the example does not include them to give you an opportunity to play with hyperparameters in the next step.<br>\n",
    "> `Pooling`: is a very common operation in CNNs and probably the most debated operation as well. Pooling reduces / condenses information by retrieving only the <code>min</code>, <code>max</code> or <code>avg</code> of a square block of information.<br>\n",
    "> `weights`: this is a hyperparameter that can be changed. You can also choose a pre-trained set of weights based on RGB images. As the input images are different to RGB and only have one channel based on VV, it is recommended not to use the pre-trained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7f76c403a370>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg = VGG16(include_top=False,\n",
    "            weights=None,\n",
    "            input_tensor=inputs,\n",
    "            pooling='avg')\n",
    "\n",
    "vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our instantiated model `vgg`, we can retrieve the input layer object with the function `input`. The resulting object is a tensor consisting of the input tensors of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 128, 128, 1) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = vgg.input\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, we add three fully-connected layers at the top of the network, in order to resemble the VGG16 architecture. The procedure is to define three `dense layers` and to add `dropout layers` in between. Dropout layers are introduced for regularisation purposes and to prevent overfitting. The procedure resembles a `stacking process` - this means, first, you build a `dense layer` followed by a `dropout layer` followed by a `dense layer` and so on. \n",
    "\n",
    "You can build a `dense layer` with the function `tf.keras.layers.Dense()` and a `dropout layer` with the function `tf.keras.layers.Dropout()`.\n",
    "\n",
    "Interpretation of the code below:\n",
    "* `dense1`: build a `dense layer` with 512 neurons on top of the `output of the vgg model` and use `relu` as activation function. `relu` stands for `Rectified Linear Unit`.\n",
    "* `dropout1`: on top of dense layer `dense1`, add a dropout layer which randomly skips half (0.5) of the 512x128 (interconnections between dense layer 1 and denser layer 2) connections\n",
    "* `dense2`: build another `dense layer` with 128 neurons on top of the dropout layer `dropout1` and use again `relu` (Rectified Linear Unit) as activation function\n",
    "* `dropout2`: on top of dense layer `dense2`, add a dropout layer which randomly skips half (0.5) of the 128x3 (interconnections between dense layer 2 and dense layer 3) connections\n",
    "* `pred`: build the final `dense layer` with 3 neurons on top of dropout layer `dropout2` and use the activation function `softmax`. The final number of neurons resembles the number of classes to predicat.\n",
    "\n",
    "The dense layer `pred` is the expected output of our model. It is a `KerasTensor` with three neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'dense_2')>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense1 = tf.keras.layers.Dense(512, activation='relu')(vgg.output)\n",
    "dropout1 = tf.keras.layers.Dropout(0.5)(dense1)\n",
    "\n",
    "dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)\n",
    "dropout2 = tf.keras.layers.Dropout(0.5)(dense2)\n",
    "\n",
    "pred = tf.keras.layers.Dense(3, activation='softmax')(dropout2)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='instantiate_keras'></a>3. Instantiate a Keras model from input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have your `VGG16 model` instantiated and on top, you added three fully-connected layers. With the KerasTensors `inp` and `pred`, you can now create the model with the function `tf.keras.Model()`. The function groups layers into an object with training and output features. The result is an untrained Neural Network, which you can configure for training in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7f76c40387f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Model(inp, pred)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the function `model.summary()` to inspect the architecture of the model. You see on top of the global average pooling layer the customized dense and dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      640       \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 15,042,243\n",
      "Trainable params: 15,042,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file='./vgg.png', show_shapes=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='configure_training'></a>4. Configure the keras model for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to configure the instantiated, but untrained Neural Network model. You can configure the model with `losses`, `optimizers` and `metrics` with the function `model.compile()`.\n",
    "\n",
    "The function takes the following kwargs:\n",
    "* `optimizer=tf.keras.optimizers.SGD()`\n",
    "  * `SGD` stands for `Gradient descent optimizer`. It is a hyperparameter which can be changed. See all optimizer options [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "* `loss=tf.keras.losses.categorical_crossentropy`\n",
    "  * `Cross-Entropy` loss is often used for multi-class classification. It means that the CNN is trained to output a probability for the specified classes for each image. It is usually combined with the activation function `softmax`.<br> \n",
    "* `metrics=['accuracy']`\n",
    "  * metrics are used to judge the performance of your model. The `accuracy` metric calculates how often the predictions equal the given labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7f76c40387f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True, name='SGD')\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='training_split'></a>5. Create training and testing subsets from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now randomly split the training data into a `training subset` and a `testing subset`. Scikit-learn offers a popular function called `train_test_split()`, which creates four subsets based on the input and output variables `X - images` and `y - labels`. The function takes the following kwargs:\n",
    "\n",
    "* `arrays`: input and output data arrays\n",
    "* `test_size`: a float number representing the proportion of the dataset to include in the test subset\n",
    "* `random_state`: An integer assuring reproducibility of the random shuffling of the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dimensions of the training and test data subsets. The test data subset consists of 25% of the total original training data (n=2805)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2103, 128, 128)\n",
      "(702, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='convert_labels'></a>One-hot encoding of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`One-hot encoding` is a common way of preprocessing categorical features for Machine Learning. The labels are represented as class numbers ranging from 0 to 2. In the one-hot encoding process, the class numbers are converted to a (binary) bitwise representation in order to avoid the algorithm assuming any sort of instrinsic hierarchy or number order.\n",
    "\n",
    "With the function `tf.keras.utils.to_categorical()`, you can convert the ship classes in a bitwise representation. You have to convert the labels for the training (`y_train`) and testing (`y_test`) subset respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='model_train'></a>*6. Optional: Train the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** the next section is optional. Training of the model might take quite a long time. For convenience, you can also go to the [next section](#load_evaluate), where we continue by loading a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can train the model, it is best to define the required callbacks. Callbacks can be passed to Keras methods such as `model.fit()` in order to be able to analyse various stages of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the callback `ModelCheckpoint` with `tf.keras.callbacks.ModelCheckpoint()` in order to save the latest Keras model according to the model's validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = tf.keras.callbacks.ModelCheckpoint('./vgg.h5',\n",
    "                                           monitor='val_accuracy',\n",
    "                                           verbose=0,\n",
    "                                           save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the callback `CSVLogger`, which saves the results of each epoch in a csv file with `tf.keras.callbacks.CSVLogger()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = tf.keras.callbacks.CSVLogger('./vgg.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the defined callbacks above, you can now train (fit) the model with the function `model.fit()`, which takes the following keyword arguments:\n",
    "* `input data`: training subset of input data - X_train\n",
    "* `target_data`: training subset of expected output (labelled) data - y_train\n",
    "* `batch_size`: integer specifying number of samples used per gradient update. Instead of the entire epoch, you only use a subset (batch) to update the network\n",
    "* `epochs`: integer specifying the number of epochs. An epoch is one iteration of the entire x and y data provided.\n",
    "* `validation_data`: data on which to evaluate the loss and any model metrics after each epoch.\n",
    "* `callbacks`: list of Keras callbacks to be applied during the training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**<br>\n",
    "The training model did only go through 2 epoch cycles, which is quite low. In general, it would be good to train the model on at least 150 epochs (training cycles). As one iteration can take quite long, it is recommend to conduct this operation on a machine with Graphics Processing Unit (GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu: 8 found\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 98s 4s/step - loss: 1.0986 - accuracy: 0.3841 - val_loss: 1.0963 - val_accuracy: 0.4017\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 95s 4s/step - loss: 1.0929 - accuracy: 0.4530 - val_loss: 1.0921 - val_accuracy: 0.4017\n",
      "Elapsed time: 196.16200350224972\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f\"cpu: {cpu_count} found\")\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=12,\n",
    "          epochs=2,\n",
    "#          epochs=150,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test),\n",
    "          steps_per_epoch = 25,\n",
    "          max_queue_size=10,\n",
    "          workers=cpu_count,\n",
    "          use_multiprocessing=cpu_count > 1,\n",
    "          callbacks=[check, log])\n",
    "\n",
    "end = timer()\n",
    "print('Elapsed time: ' + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='load_evaluate'></a>7. Load and evaluate the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the function `load_model` from `tf.keras.model` to load the pre-trained model `vgg.h5`. The function `model.evaluate()` lets you evaluate the performance of the trained model with the testing subsets of `input` and `target` data. It returns the frequency of how often the predicted values have matched the test values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 31s 1s/step - loss: 1.0963 - accuracy: 0.4017\n",
      "Test_acc: 0.4017\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./vgg.h5')\n",
    "\n",
    "_, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test_acc: %.4f' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, you can apply the function `model.predict()` to predict a ship class with the help of the trained model. You provide the function an input array from the test dataset (`X_test`). The result is a probability vector, indicating for each ship class the probability. The highest probability is the predicted class. Feel free to change the index in order to get a better feeling of the robustness of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Bulk Carrier', 'Container Ship', 'Tanker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3412053  0.3278484  0.33094627]]\n"
     ]
    }
   ],
   "source": [
    "index = 50\n",
    "pred = model.predict(np.array([X_test[index]]))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a last step, based on the probability vector above, you can compare the predicted class (`np.argmax(pred)`) against the actual class of a given input image (`np.argmax(y_test[])`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to predict: Bulk Carrier\n",
      "Predicted with label: Bulk Carrier\n"
     ]
    }
   ],
   "source": [
    "print('Class to predict: ' + class_names[np.argmax(y_test[index])])\n",
    "print('Predicted with label: ' +class_names[np.argmax(pred)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.futurelearn.com/courses/artificial-intelligence-for-earth-monitoring/1/steps/1171726\"><< Back to FutureLearn</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../img/copernicus_logo.png' alt='Copernicus logo' align='left' width='20%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course developed for [EUMETSAT](https://www.eumetsat.int/), [ECMWF](https://www.ecmwf.int/) and [Mercator Ocean International](https://www.mercator-ocean.fr/en/) in support of the [EUâ€™s Copernicus Programme](https://www.copernicus.eu/en) and the [WEkEO platform](https://wekeo.eu/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
