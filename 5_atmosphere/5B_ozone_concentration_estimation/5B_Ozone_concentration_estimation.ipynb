{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<center><img src='../../img/ai4eo_logos.jpg' alt='Logos AI4EO MOOC' width='80%'></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.futurelearn.com/courses/artificial-intelligence-for-earth-monitoring/1/steps/1280523\"><< Back to FutureLearn</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of a Neural Network for estimating total column ozone from EO data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>by Davide De Santis, University of Tor Vergata, Rome, Italy</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the video tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<div align=\"center\"><iframe src=\"https://player.vimeo.com/video/631907302?h=3debddcc22\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen align=\"middle\"></iframe></div>')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow guides you through the process of training a `neural network` based on [Sentinel-5P/TROPOMI](http://www.tropomi.eu/) data in order to predict `total column ozone concentration`. The labelled data base on a Sentinel-5P/TROPOMI scene which was acquired over Sicily (Italy) on December 29, 2018.\n",
    "\n",
    "The labelled data takes information from the following Sentinel-5P/TROPOMI data:\n",
    "* [Sentinel-5P/TROPOMI Level 2 - Product User Manual - Ozone Total Column](https://sentinels.copernicus.eu/documents/247904/2474726/Sentinel-5P-Level-2-Product-User-Manual-Ozone-Total-Column.pdf/6d813a34-2811-4a29-9f79-b6a0dda95901?t=1611302543710)\n",
    "* [Sentinel-5P/TROPOMI Level 1B -  Product Readme File](https://sentinels.copernicus.eu/documents/247904/3541451/Sentinel-5P-Level-1b-Product-Readme-File.pdf/a89d82ce-7414-43e6-ac77-0c371ed1b096?t=1572967657000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks have shown promising results for atmospheric inversion problems, in particular for ozone column abundance estimation. For this reason, the workflow trains a [sequential neural network with Keras](https://keras.io/api/models/sequential/), which is a linear stack of layers. There are two ways to build Keras models:\n",
    "* in a `sequential` way and\n",
    "* a `functional` way.\n",
    "\n",
    "The `sequential` API allows you to create models in a sequential way, which means you add one layer after the other. A sequential model is applicable for most problems. \n",
    "\n",
    "The network is designed with 23 input nodes (spectral radiances and zenith angles), a single hidden layer consinsting of 20 neurons and 1 output node (ozone total column concentration) both with a [sigmoidal activation function](https://keras.io/api/layers/activations/) associated.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow makes use of the .csv file [AI4EO_Ozone_Estimation_Sentinel-5P](./AI4EO_Ozone_Estimation_Sentinel-5p.csv), which contains labelled data from a Sentinel-5P/TROPOMI scene acquired over Sicily on December 29, 2018.\n",
    "\n",
    "The file contains 2500 row entries and 26 columns. The 2500 rows resemble 2500 point locations across Sicily, Italy. The 26 columns represent the following information:\n",
    "* Column 1-21: `Spectral radiance value for 21 wavelengths [rad_325.0_nm, rad_335.0_nm]` sensitive to the ozone column abundance and derived from the TROPOMI Level-1B Band-3 product\n",
    "* Column 22: `solar zenith angle [sza]` derived from the TROPOMI Level-1B Band-3 product\n",
    "* Column 23: `sensor zenith angle [vza]` derived from the TROPOMI Level-1B Band-3 product\n",
    "* Column 24: `latitude [lat]`\n",
    "* Column 25: `longitude [lon]`\n",
    "* Column 26: `Total Ozone column abundance [ozone_total_column_[DU]]` derived from the TROPOMI Level-2 Total Column Ozone product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Application of neural algorithms for a real-time estimation of ozone profiles from GOME measurements](https://ieeexplore.ieee.org/document/1105913)\n",
    "* [Tropospheric ozone column retrieval at northern mid-latitudes from the Ozone Monitoring Instrument by means of a neural network algorithm](https://amt.copernicus.org/articles/4/2375/2011/)\n",
    "* [Tropospheric Ozone Column Retrieval From ESA-Envisat SCIAMACHY Nadir UV/VIS Radiance Measurements by Means of a Neural Network Algorithm, in IEEE Transactions on Geoscience and Remote Sensing](https://ieeexplore.ieee.org/iel5/36/4358825/06008635.pdf?casa_token=lqc9umtUPLEAAAAA:IjRD1HqEc0H3BeSNdFSr-AVWliUSUUTfxTLGdT9fGRJKADbxf-sIS5uDn-ISDyFPXnJtA7m3Ug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook outline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1 - Prepare input and output data for model training](#input_output_ozone)\n",
    "* [2 - Define and compile a sequential neural network with Keras](#model_definition_ozone)\n",
    "* [3 - Training (fitting) of the sequential neural network](#fitting_ozone)\n",
    "* [4 - Use the trained sequential neural network to predict total column ozone estimates](#predict_ozone)\n",
    "* [5 - Evaluate the accuracy of the model predictions](#evaluate_ozone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## BEGIN S3FS IMPORT SNIPPET ##\n",
    "import os, sys\n",
    "s3_home =  os.getcwd()\n",
    "try: sys.path.remove(s3_home) # REMOVE THE S3 ROOT FROM THE $PATH\n",
    "except Exception: pass\n",
    "current_dir = os.getcwd()\n",
    "os.chdir('/home/jovyan') # TEMPORARILY MOVE TO ANOTHER DIRECTORY\n",
    "\n",
    "# BEGIN IMPORTS #\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# END IMPORTS #\n",
    "\n",
    "os.chdir(current_dir) # GO BACK TO YOUR PREVIOUS DIRECTORY\n",
    "sys.path.append(s3_home) # RESTORE THE S3 ROOT IN THE $PATH\n",
    "## END S3FS IMPORT SNIPPET ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='input_output_ozone'></a>1. Prepare input and output data for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the training dataset [AI4EO_Ozone_Estimation_Sentinel-5p](./AI4EO_Ozone_Estimation_Sentinel-5p.csv), which is provided as a `.csv` file. The file contains 2500 row entries and 26 columns. The 2500 rows resemble 2500 point locations across Sicily, Italy. The 26 columns represent the following information:\n",
    "* Column 1-21: `Spectral radiance value for 21 wavelengths [rad_325.0_nm, rad_335.0_nm]` sensitive to the ozone column abundance and derived from the TROPOMI Level-1B Band-3 product\n",
    "* Column 22: `solar zenith angle [sza]` derived from the TROPOMI Level-1B Band-3 product\n",
    "* Column 23: `sensor zenith angle [vza]` derived from the TROPOMI Level-1B Band-3 product\n",
    "* Column 24: `latitude [lat]`\n",
    "* Column 25: `longitude [lon]`\n",
    "* Column 26: `Total Ozone column abundance [ozone_total_column_[DU]]` derived from the TROPOMI Level-2 Total Column Ozone product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Python library [pandas](https://pandas.pydata.org/) and the function `read_csv()`, you can easily read a `.csv` file and inspect the content of the file. The function `.shape()` shows you that the loaded data file has 2500 rows and 26 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./AI4EO_Ozone_Estimation_Sentinel-5p.csv\") \n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input and output variables of the neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, you define the columns and rows of the loaded dataset that shall serve as input (X) and ouput (y) variables of the neural network model. The first 23 columns featuring spectral radiance values for 21 wavelengths as well as solar and sensor zenith angles shall serve as input (X) variables. The total column ozone information serves as output (y) variable, which the model shall be able to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = dataset.values[:, 0:23]\n",
    "y = dataset.values[:, 25:26]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset in training, validation and testing subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can split the input and output variables into subsets for `training`, `validation` and `testing`:\n",
    "* `training data`: is the actual sample used to train the Machine Learning model that learns from this data\n",
    "* `validation data`: used to evaluate how well the model is fitted onto the training data, while the model hyperparameters are tuned. However, the network does not learn from this data\n",
    "* `test data`: used to provide an evaluation of the final model fit and these data are only used when a model is completely trained based on training and validation data\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/) offers a function called `train_test_split()` which creates four subsets based on the input and output variables, `X` and `y` respectively. \n",
    "\n",
    "The function takes the following kwargs:\n",
    "* `arrays`: input and output data arrays\n",
    "* `test_size`: a float number representing the proportion of the dataset to include in the test subset\n",
    "* `random_state`: An integer assuring reproducibility of the random shuffling of the data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you want to split the data into 60% training and 40% validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, y_train, y_val_and_test = train_test_split(X, \n",
    "                                                                    y, \n",
    "                                                                    test_size=0.4, \n",
    "                                                                    random_state=32)\n",
    "\n",
    "X_train.shape, X_val_and_test.shape, y_train.shape, y_val_and_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a following step, you want split the validation and test data subset and use 55% for validation and 45% for testing the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_val_and_test, \n",
    "                                                y_val_and_test, \n",
    "                                                test_size=0.45, \n",
    "                                                random_state=32)\n",
    "\n",
    "X_val.shape, X_test.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation of input and output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to normalise the input and output data. `Normalisation` is a common pre-processing step in Machine Learning to bring variables that are measured in different units to a common scale. Normalisation is not needed for all Machine Learning algorithms, but it is critical for `neural networks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `MinMaxScaler`class from the scikit-learn library to scale the input and output variables. You can specify the minimum and maximum bounds of the `feature_range` as keyword argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define two `MinMaxScaler` objects, for the input and output data respectively with different feature ranges:\n",
    "* `input_scaler`: [-5, 5]\n",
    "* `output_scaler`: [0, 1]\n",
    "\n",
    "The feature range is adjusted to the `sigmoidal activation function` used for each neuron in the neural network. The input and output features are scaled according to the function domain and co-domain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input_scaler = MinMaxScaler(feature_range=(-5, 5))\n",
    "output_scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to fit the `MinMaxScaler` object to the data space. You can fit the two `MinMaxScaler` objects above to the input (`X`) and output (`y`) data spaces.\n",
    "\n",
    "Once the `MinMaxScaler` object are fitted, you can scale (transform) the input and output data subsets for `training`, `validation` and `testing` with the function `transform()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Since we do not use the test data subset during the training phase, it is not required to scale the variable `y_test`. We only need the data in the original scale to compare them with the output of the network in order to estimate the accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input_scaler.fit(X)\n",
    "X_train_scaled = input_scaler.transform(X_train)\n",
    "X_val_scaled = input_scaler.transform(X_val)\n",
    "X_test_scaled = input_scaler.transform(X_test)\n",
    "\n",
    "# with the output scaler we fit all the output space and then scale each splitted part of the dataset used as output for the neural network\n",
    "output_scaler.fit(y)\n",
    "y_train_scaled = output_scaler.transform(y_train)\n",
    "y_val_scaled = output_scaler.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='model_definition_ozone'></a>2. Define and compile a sequential neural network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, you can define the sequential neural network with `keras.Sequential()`. The aim is to define a neural network with 23 input nodes (spectral radiances and zenith angles), a single hidden layer consisting of 20 neurons and 1 output node (total column ozone concentration). The input dimension has to match with the number of columns of the input (`X`) data.\n",
    "\n",
    "You want to add the following `dense layers`:\n",
    "* Layer 1: `dense layer` with 20 neurons, 23 input dimensions and an associated `sigmoid` activation function\n",
    "* Layer 2: `dense layer` with 1 final neuron and an associated `sigmoid` activation function\n",
    "\n",
    ">**Note:**\n",
    "See an overview of possible layer activation functions [here](https://keras.io/api/layers/activations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_nn_for_o3 = Sequential([Dense(units=20, \n",
    "                                    activation='sigmoid', \n",
    "                                    input_shape=(23,)),\n",
    "                              Dense(units=1, \n",
    "                                    activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before model fitting is to compile (configure) the model. You can do this with the `.compile()` function. Let us define the following configurations:\n",
    "* `optimizer='sgd'` - use a `stochastic gradient descent` as [optimizer](https://keras.io/api/optimizers/)\n",
    "* `loss='mean_squared_error'` - use mean squared error as [loss function](https://keras.io/api/losses/)\n",
    "* `metrics=['mae']` - Use mean absolute error as a [metric](https://keras.io/api/metrics/) of model performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_nn_for_o3.compile(optimizer='sgd',  \n",
    "                        loss='mean_squared_error',  \n",
    "                        metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='fitting_ozone'></a>3. Training (fitting) of the sequential neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Model fitting` is the process that trains the sequential neural network with the defined training input and output data subsets. You can specify the following keyword arguments:\n",
    "* `input (X)` and `output (y)` data: here specify the input and output training data for the model\n",
    "* `validation_data`: here we enter the validation data subsets `X_val_scaled` and `y_val_scaled` and our model outputs are validated against these validation data after each epoch (training cycle)\n",
    "* `epochs`: number of training cycles\n",
    "* `batch_size`: defines the size of a training data subset (e.g. 10 samples) after which the weights of the network are updated\n",
    "* `verbose`: set verbose=1 to print the scores after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    "The output of the training process is a `history` object and for this reason, the output object has the name `hist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start_training = time.time()\n",
    "\n",
    "hist = model_nn_for_o3.fit(X_train_scaled, \n",
    "                           y_train_scaled, \n",
    "                           batch_size=10, \n",
    "                           epochs=300, \n",
    "                           validation_data=(X_val_scaled, y_val_scaled), \n",
    "                           verbose=1)\n",
    "\n",
    "end_training = time.time()\n",
    "print(\"Training time \", round((end_training - start_training), 3), \"s\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful evaluation of the training process is to plot the `loss function` and `mean absolute error` during the training epochs. A decreasing trend of both metrics indicates the capability of the network to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the `loss function` during the training epochs by plotting `loss` and `val_loss` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the `mean absolute error` during the training epochs by plotting `mae` and `val_mae` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['mae'])\n",
    "plt.plot(hist.history['val_mae'])\n",
    "plt.title('Model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='predict_ozone'></a>4. Use the trained sequential neural network to predict total column ozone estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the trained model to predict the output variable `total_column_ozone`, based on the input data subsets `X_train_scaled`, `X_val_scaled` and `X_test_scaled`. You can use the function `model.predict()` to do so. The result is the predicted `total_column_ozone`, but still in the normalized / transformed format. For this reason, in the next step we have to inverse the transformation.\n",
    "\n",
    "> **Note:** The estimation of the accuracy on training and validation data can be compared with that obtained on the test data. This gives us information about the variation of the error when we generalize the network performance over data that was not considered during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "predict_train = model_nn_for_o3.predict(X_train_scaled)\n",
    "predict_val = model_nn_for_o3.predict(X_val_scaled)\n",
    "predict_test = model_nn_for_o3.predict(X_test_scaled)\n",
    "predict_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MinMaxScaler` class has a function called `inverse_transform()`, which allows you to invert the normalisation process. The function takes the normalised predicted values and returns the converted `total column ozone` estimates in [`Dobson Unit (DU)`](https://ozonewatch.gsfc.nasa.gov/facts/dobson_SH.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "predict_train_inverse = output_scaler.inverse_transform(predict_train)\n",
    "predict_val_inverse = output_scaler.inverse_transform(predict_val)\n",
    "predict_test_inverse = output_scaler.inverse_transform(predict_test)\n",
    "predict_train_inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the function `model.save()` to save the Keras model in the `Keras H5` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_nn_for_o3.save(\"../../../../model_nn_for_o3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='evaluate_ozone'></a>5. Evaluate the accuracy of the model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, you want to evaluate the accuracy of your trained model. There are different metrics suitable for `regression problems performance evaluation` and [here](https://scikit-learn.org/stable/modules/classes.html#regression-metrics), you find a list of `regression metrics` supported by [scikit-learn](https://scikit-learn.org/stable/). For this workflow, we focus on the metrics `mean absolute error`, `mean_squared_error` and `Persons's correlation coefficient`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with computing the `mean absolute error` between estimated and true ozone concentration values for the three data subsets *`_train`*, *`_val`* and *`_test`*. From the metrics class of the scikit-learn package, you can use the function `mean_absolute_error()` and provide the true and predicted ozone concentration values as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mae_train = mean_absolute_error(y_train, predict_train_inverse)\n",
    "mae_val = mean_absolute_error(y_val, predict_val_inverse)\n",
    "mae_test = mean_absolute_error(y_test, predict_test_inverse)\n",
    "\n",
    "print(f'Train MAE: {round(mae_train, 3)} DU, Val MAE: {round(mae_val, 3)} DU, Test MAE: {round(mae_test, 3)} DU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us compute the `root mean squared error` between estimated and true ozone concentration values, again for the three data subsets *`_train`*, *`_val`* and *`_test`*. The metrics class of the scikit-learn package offers the function `mean_square_error()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rmse_train = mean_squared_error(y_train, predict_train_inverse, squared=False)\n",
    "rmse_val = mean_squared_error(y_val, predict_val_inverse, squared=False)\n",
    "rmse_test = mean_squared_error(y_test, predict_test_inverse, squared=False)\n",
    "\n",
    "print(f'Train RMSE: {round(rmse_train, 3)} DU, Val RMSE: {round(rmse_val, 3)} DU, Test RMSE: {round(rmse_test, 3)} DU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us compute `Pearson's correlation coefficient` between the estimated and true ozone concentration values, again for the three data subsets *`_train`*, *`_val`* and *`_test`*. scikit-learn offers the function `r2_score()`, which calculates the coefficient of determination (R<sup>2</sup>), which is the square of Pearson's correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pearson_train = np.sqrt(r2_score(y_train, predict_train_inverse))\n",
    "pearson_val = np.sqrt(r2_score(y_val, predict_val_inverse))\n",
    "pearson_test = np.sqrt(r2_score(y_test, predict_test_inverse))\n",
    "\n",
    "print(f'Train Pearson: {round(pearson_train, 3)}, Val Pearson: {round(pearson_val, 3)}, Test Pearson: {round(pearson_test, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final evaluation performance, let us compare the `mean` and `standard deviation (std)` of the estimated vs real total column ozone values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" \"Evaluation of mean and standard deviation of the estimated values of ozone total column compared to the actual ones\")\n",
    "print(f'Ozone Total Column true mean: {round(np.mean(y_test), 3)} DU --- Ozone Total Column estimated mean: {round(float(np.mean(predict_test_inverse)), 3)} DU')\n",
    "print(f'Ozone Total Column true std: {round(np.std(y_test), 3)} DU --- Ozone Total Column estimated std: {round(float(np.std(predict_test_inverse)), 3)} DU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Conclusion:**<br>\n",
    "To evaluate the performance, we can compare the mean absolute error (MAE) and mean on the test data subset. The `MAE` for the test data is much higher. Another possibility is to compare the root mean squared error (RMSE) with the standard deviation (which represents the \"average\" estimator). You can see that in the present workflow we could obtain a significant reduction of the RMSE with respect to the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.futurelearn.com/courses/artificial-intelligence-for-earth-monitoring/1/steps/1174962\"><< Back to FutureLearn</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../img/copernicus_logo.png' alt='Copernicus logo' align='left' width='20%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course developed for [EUMETSAT](https://www.eumetsat.int/), [ECMWF](https://www.ecmwf.int/) and [Mercator Ocean International](https://www.mercator-ocean.fr/en/) in support of the [EUâ€™s Copernicus Programme](https://www.copernicus.eu/en) and the [WEkEO platform](https://wekeo.eu/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
